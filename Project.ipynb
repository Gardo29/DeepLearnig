{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorenzo-Gardini/DeepLearnig/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiOXJHJFcQVq"
      },
      "source": [
        "# Authors\n",
        "The authors of this code, this notebook and its documentation are [Lorenzo Gardini](mailto: lorenzo.gardini7@studio.unibo.it) and [Vlad Mattiussi](mailto: vlad.mattiussi@studio.unibo.it)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv58xcBkih2P"
      },
      "source": [
        "prendi spunto da https://colab.research.google.com/drive/1f9BxFxPxhPKUba8Ifcq4juv042ZA484-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAC5ZN67Zdw9"
      },
      "outputs": [],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OG5uwXOwOQk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 pyvirtualdisplay tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gym\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ],
      "metadata": {
        "id": "ha2SdD91I6Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisi del problema\n",
        "\n",
        "il nostro obiettivo è quello di insegnare alla macchina a giocare correttamente al gioco \"Pong\" per consolle Atari (https://en.wikipedia.org/wiki/Pong). L'obiettivo del gioco è quello di colpire una palla, simulata da un piccolo quadrato, e rispedirla dall'altra parte: il giocatore che non riesce a rispedire la palla concede un punto al suo avversario. Il primo giocatore che raggiunge 21 punti vince la partita.\n",
        "Per risolvere il problema si propone di ricorrere agli strumenti di reinforcement learning. Quindi si pensa di addestrare un modello basato su alcuni algoritmi. Gli algoritmi utilizzati saranno DQN, DDQN, con experience replay, con prioritized experience replay e Dueling DQN sia di studiare ed utilizzare algoritmi policy based (PPO e A2C).\n",
        "\n",
        "<img src=https://www.gymlibrary.dev/_images/pong.gif width=\"300\">\n",
        "\n",
        "**Ambiente d'addestramento:**\n",
        "\n",
        "Per l'ambiente di addestramento utiliziamo la libreria Gym, la quale ci fornisce il gioco di Pong.\n",
        "Gym implements the classic “agent-environment loop”: The agent performs some actions in the environment (usually by passing some control inputs to the environment, e.g. torque inputs of motors) and observes how the environment’s state changes. One such action-observation exchange is referred to as a timestep.\n",
        "\n",
        "<img src=https://www.gymlibrary.dev/_images/AE_loop_dark.png width=\"500\">\n",
        "\n",
        "The goal in RL is to manipulate the environment in some specific way. For instance, we want the agent to navigate a robot to a specific point in space. If it succeeds in doing this (or makes some progress towards that goal), it will receive a positive reward alongside the observation for this timestep. The reward may also be negative or 0, if the agent did not yet succeed (or did not make any progress). The agent will then be trained to maximize the reward it accumulates over many timesteps.\n",
        "\n",
        "Spaces are usually used to specify the format of valid actions and observations. Every environment should have the attributes action_space and observation_space, both of which should be instances of classes that inherit from Space. There are multiple Space types available in Gym, nel nostro caso sara' discreto, cioe' describes a discrete space where {0, 1, …, n-1} are the possible values our observation or action can take. Values can be shifted to {a, a+1, …, a+n-1} using an optional argument. Le azioni di Pong saranno 3: su, giu e stare fermi. Per quanto riguarda l'osseravzione by default, the environment returns the RGB image that is displayed to human players as an observation. However, nel nostro caso sara' una immagine in grayscale (Box([[0 ... 0] ... [0  ... 0]], [[255 ... 255] ... [255  ... 255]], (250, 160), uint8)).\n",
        "\n",
        "Per l'addestramento si è utilizzato la libreria Stable-Baseline con alcune modifiche. Abbiamo modificato il codice della libreria estendendo le classi principali in modo da poter usare gli stessi parametri e struttura della rete usati da deep mind.\n",
        "\n"
      ],
      "metadata": {
        "id": "TQWKju7kmGuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "px36N3VnoP-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import abstractmethod\n",
        "import random\n",
        "import time\n",
        "import random\n",
        "import statistics\n",
        "\n",
        "# keras and tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.losses import Huber \n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# save model\n",
        "import pickle\n",
        "\n",
        "# video and images manipulation\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "from gym.wrappers import RecordVideo\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "from skimage import transform\n",
        "from skimage.color import rgb2gray\n"
      ],
      "metadata": {
        "id": "X3OkyeKAoRfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skbwoNDholX7"
      },
      "source": [
        "## COMMON CONSTANTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09WsbkO7cOMr"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"ALE/Pong-v5\"\n",
        "data_path = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMCcMqvOK_UV"
      },
      "source": [
        "### Display functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqeEJi9SSDCy"
      },
      "outputs": [],
      "source": [
        "VIDEO_FOLDER = \"./video\"\n",
        "\n",
        "def make_env():\n",
        "  return gym.make(ENV_NAME)\n",
        "\n",
        "def play_env(policy, steps):\n",
        "  env = wrap_video_env(make_env())\n",
        "  terminated = False\n",
        "  observation = env.reset()\n",
        "\n",
        "  for _ in range(steps):\n",
        "    observation, reward, terminated, _ = env.step(policy(observation))\n",
        "    if terminated:\n",
        "      break\n",
        "  env.close()\n",
        "  \n",
        "def wrap_video_env(env):\n",
        "  return RecordVideo(env, VIDEO_FOLDER)\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4 = open(VIDEO_FOLDER + \"/rl-video-episode-0.mp4\",'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  display(HTML(f\"\"\" <video controls autoplay><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8T5zvjK4c2Q"
      },
      "source": [
        "## DQN architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abbiamo utilizzato due modelli diversi di architettura DQN: uno \"classico\" e uno dueling. Di seguito mostrati in ordine."
      ],
      "metadata": {
        "id": "qfFvTEG7w6eb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3U0UE2ZTfq1"
      },
      "source": [
        "The following image shows the architecture of DQN.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/DQN_architecture.png width=\"1200\">\n",
        "\n",
        "Keras offers a wide range of built-in layers ready for use, including:\n",
        "- [**Conv2D**](https://keras.io/api/layers/convolution_layers/convolution2d/) - a 2D convolution layer;\n",
        "- [**Flatten**](https://keras.io/api/layers/reshaping_layers/flatten/) - a simple layer used to flatten the input."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dueling DQN**:\n",
        "\n",
        "This algorithm splits the Q-values in two different parts, the value function V(s) and the advantage function A(s, a).\n",
        "\n",
        "The value function V(s) tells us how much reward we will collect from state s. And the advantage function A(s, a) tells us how much better one action is compared to the other actions. Combining the value V and the advantage A for each action, we can get the Q-values:\n",
        "\n",
        "<img src=https://miro.medium.com/max/720/1*81-seZY1rVwC0wzXBprFJg.png width=\"300\" height= \"50\">\n",
        "\n",
        "What the Dueling DQN algorithm proposes is that the same neural network splits its last layer in two parts, one of them to estimate the state value function for state s (V(s)) and the other one to estimate the advantage function for each action a (A(s, a)), and at the end it combines both parts into a single output, which will estimate the Q-values. This change is helpful, because sometimes it is unnecessary to know the exact value of each action, so just learning the state-value function can be enough is some cases.\n",
        "\n",
        "<img src=https://miro.medium.com/max/720/1*vkrLw_sgOgFeBUuZylhyFA.png width=\"800\" height= \"400\">\n",
        "\n",
        "However, training the neural network by simply summing the value and advantage functions is not possible. In Q=V+A, given the function Q, we cannot determine the values of V and A, since that is “unidentifiable”. To solve this we can do this: force the highest Q-value to be equal to the value V, thus making the highest value in the advantage function be zero and all other values negative. This will tell us exactly the value for V, and we can calculate all the advantages from there, solving the problem. This is how we would train it:\n",
        "\n",
        "<img src=https://miro.medium.com/max/720/1*RT7XmyYC8y9uI61tNB1KOQ.png width=\"500\" height= \"50\">\n",
        "\n"
      ],
      "metadata": {
        "id": "uzqheu7dnQ-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_data(values, path):\n",
        "  try:\n",
        "    with open(path, 'wb') as fp:\n",
        "      pickle.dump(values, fp, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "      return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "def load_data(path):\n",
        "  try:\n",
        "    with open(path, \"rb\") as parameters:\n",
        "      return pickle.load(parameters)\n",
        "  except:\n",
        "      return {}"
      ],
      "metadata": {
        "id": "4QVgwyHwf0g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzwBTMVE9RAk"
      },
      "outputs": [],
      "source": [
        "def build_dqn(input_shape=(84, 84, 4), action_count=3):\n",
        "  model=keras.Sequential(\n",
        "          [\n",
        "            layers.Input(shape=input_shape,name='input'),\n",
        "            layers.Conv2D(filters=32, kernel_size=8, strides=4,activation='relu',padding='valid',name='c1'),\n",
        "            layers.Conv2D(filters=64, kernel_size=4,strides=2,activation='relu',padding='valid',name='c2'),\n",
        "            layers.Conv2D(filters=64, kernel_size=3,strides=1,activation='relu',padding='valid',name='c3'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation='relu',name='fc'),\n",
        "            layers.Dense(units=action_count,activation='linear',name='output')\n",
        "          ]\n",
        "      )\n",
        "  model.compile()\n",
        "  return model\n",
        "\n",
        "def build_dueling_dqn(input_shape=(84, 84, 4), action_count=3):\n",
        "  input = layers.Input(shape=input_shape,name='input')\n",
        "  conv_1 = layers.Conv2D(filters=32, kernel_size=8, strides=4,activation='relu',padding='valid')(input)\n",
        "  conv_2 = layers.Conv2D(filters=64, kernel_size=4,strides=2,activation='relu',padding='valid')(conv_1)\n",
        "  conv_3 = layers.Conv2D(filters=64, kernel_size=3,strides=1,activation='relu',padding='valid')(conv_2)\n",
        "  flatten = layers.Flatten()(conv_3)\n",
        "  # TODO AGGIUNGI FULLY CONNECTED DOPO FLATTEN\n",
        "\n",
        "  state = layers.Dense(1,activation='linear',name='state')(flatten)\n",
        "  raw_advantages = layers.Dense(action_count,activation='linear')(flatten)\n",
        "\n",
        "  advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1, keepdims=True)\n",
        "  Q_values = state + advantages\n",
        "  model = tf.keras.Model(inputs=[input], outputs=[Q_values])\n",
        "  model.compile()\n",
        "  return model\n",
        "\n",
        "def build_action_target_dqn(input_shape=(84, 84, 4), action_count=3):\n",
        "  action_model = build_dqn(input_shape, action_count)\n",
        "  target_model = build_dqn(input_shape, action_count)\n",
        "  target_model.set_weights(action_model.get_weights())\n",
        "  return action_model, target_model\n",
        "\n",
        "def build_dueling_action_target_dqn(input_shape=(84, 84, 4), action_count=3):\n",
        "  action_model = build_dueling_dqn(input_shape, action_count)\n",
        "  target_model = build_dueling_dqn(input_shape, action_count)\n",
        "  target_model.set_weights(action_model.get_weights())\n",
        "  return action_model, target_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mffBPgJ7zS5"
      },
      "source": [
        "## Frame preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrcF-baLSPbG"
      },
      "source": [
        "To reduce the state complexity, and consequently the computation time, each frame is:\n",
        "1. transformed in grayscale;\n",
        "2. cropped to select the region of interest;\n",
        "3. resized to 84×84.\n",
        "\n",
        "Moreover, to reduce the memory occupation of the replay buffer, pixel values are stored as bytes (in the range [0;255]) and converted in floating-point values (in the range [0;1]) only when needed as input for the *action* or *target* models.\n",
        "\n",
        "The following function performs such operations, given:\n",
        "- the input frame (*frame*); \n",
        "- the coordinates of the region of interest (*top_crop*, *bottom_crop*, *left_crop* and *right_crop*);\n",
        "- the dimension of the resided frame (*resized_shape*).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AriM_6qYQoI"
      },
      "outputs": [],
      "source": [
        "TOP_CROP = 14\n",
        "BOTTOM_CROP = 196\n",
        "LEFT_CROP = 8\n",
        "RIGHT_CROP = -1\n",
        "RESIZE_SHAPE = (84,84)\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "  # 1. the input RGB frame is transformed in grayscale\n",
        "  gray = rgb2gray(frame)\n",
        "      \n",
        "  # 2. the region of interest is cropped\n",
        "  cropped_frame = gray[TOP_CROP:BOTTOM_CROP, LEFT_CROP:RIGHT_CROP]\n",
        "  \n",
        "  # 3. the resulting images is resized\n",
        "  preprocessed_frame = transform.resize(cropped_frame, RESIZE_SHAPE)\n",
        "  \n",
        "  # convert float to byte\n",
        "  byte_preprocessed_frame=(preprocessed_frame*255).astype('uint8')\n",
        "\n",
        "  return byte_preprocessed_frame\n",
        "\n",
        "def scale_frames(frames):\n",
        "  return frames / 255.0\n",
        "\n",
        "def preprocessed_env():\n",
        "  return AtariWrapper(make_env())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1EmO4jRpHH2"
      },
      "source": [
        "### Visualize preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = preprocessed_env()\n",
        "plt.axis('off')\n",
        "frame = env.reset()\n",
        "print(frame.squeeze().shape)\n",
        "plt.imshow(frame.squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RtJoOffNPtQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "2e8d2a53-8acb-4df9-f502-301e5cb9b4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NamespaceNotFound",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNamespaceNotFound\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7084b3f68dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-4bf0492d5bd7>\u001b[0m in \u001b[0;36mpreprocessed_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessed_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mAtariWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3d23baa5ec1b>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    183\u001b[0m             ]\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 raise error.DeprecatedEnv(\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \"Env {} not found (valid versions include {})\".format(\n\u001b[1;32m    187\u001b[0m                         \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_namespace_exists\u001b[0;34m(ns)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;34m\"KellyCoinflipGeneralized\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;34m\"NChain\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0;34m\"Roulette\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;34m\"GuessingGame\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"HotterColder\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNamespaceNotFound\u001b[0m: Namespace ALE not found. Have you installed the proper package for ALE?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUhnOBQ6pGHw"
      },
      "outputs": [],
      "source": [
        "env = make_env()\n",
        "plt.axis('off')\n",
        "frame = env.reset()\n",
        "preprocessed_frame = preprocess_frame(frame)\n",
        "print(preprocessed_frame.shape)\n",
        "plt.imshow(preprocessed_frame, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CENgNMQVz8YK"
      },
      "source": [
        "## Frame Stacking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STNUjCIUTCnu"
      },
      "source": [
        "To solve the problem of temporal limitation and give the network the sense of motion, DQN takes a stack of frames as input.\n",
        "\n",
        "The following function stacks frames together given:\n",
        "- the new frame to add (*new_frame*);\n",
        "- the number of frames to stack (*frame_count*);\n",
        "- the previous frames contained in a **deque** object (*deque_frames*).\n",
        "\n",
        "For the first *new_frame*, *frame_count* frames identical to *new_frame* are added to a new **deque** object. Otherwise, the *new_frame* is appended to the deque that automatically removes the oldest frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qABySj6nrgK-"
      },
      "outputs": [],
      "source": [
        "class FramesStack:\n",
        "  def __init__(self, initial_frame, frame_count, preprocess_frame):\n",
        "    self._frame_count = frame_count\n",
        "    self._preprocess_frame = preprocess_frame\n",
        "\n",
        "    preprocessed_initial_frame = preprocess_frame(initial_frame)\n",
        "    self._deque = deque([preprocessed_initial_frame for _ in range(frame_count)], maxlen=frame_count)\n",
        "  \n",
        "  def add(self, frame):\n",
        "    self._deque.append( self._preprocess_frame(frame) )\n",
        "    return self\n",
        "  \n",
        "  def stack(self):\n",
        "    return np.stack(self._deque, axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNk5xCxRFHwK"
      },
      "source": [
        "## Replay memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feB3RlCDUPIq"
      },
      "source": [
        "To remove correlations between consecutive transitions and make the DQN training more stable, the *experience replay* technique is used.\n",
        "\n",
        "The replay memory is implemented as a [**deque**](https://docs.python.org/3/library/collections.html#collections.deque) (Doubly Ended Queue) object providing an O(1) time complexity for append and pop operations from both the ends of the queue. Moreover, if the *maxlen* parameter is specified, the **deque** is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end.\n",
        "\n",
        "The DQN authors suggest to populate the replay memory before starting the learning process. For each step $t$, a random action is chosen and executed then the transition $<s_t,a_t,r_{t+1},s_{t+1}>$ is stored in the replay memory. The following function initializes the replay memory given:\n",
        "- the environment (*env*);\n",
        "- the replay memory (*replay_memory*);\n",
        "- the number of transitions to be stored in the replay memory (*replay_memory_init_size*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prioritized experience replay**:\n",
        "\n",
        "It is built on top of experience replay buffers, which allow a reinforcement learning (RL) agent to store experiences in the form of transition tuples, usually denoted as (st,at,rt,st+1) with states, actions, rewards, and successor states at some time index t. In contrast to consuming samples online and discarding them thereafter, sampling from the stored experiences means they are less heavily “correlated” and can be re-used for learning.\n",
        "\n",
        "Uniform sampling from a replay buffer is a good default strategy, and probably the first one to attempt. But prioritized sampling, as the name implies, will weigh the samples so that “important” ones are drawn more frequently for training."
      ],
      "metadata": {
        "id": "dAnshen44JR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSVjR2zbwui3"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "  @abstractmethod\n",
        "  def add_transition(self, transition):\n",
        "    pass\n",
        "\n",
        "  def _split_batch(self, batch):\n",
        "    states, actions, rewards, next_states, dones =  [\n",
        "            np.array([experience[field_index] for experience in batch])\n",
        "            for field_index in range(5) \n",
        "          ]\n",
        "    return scale_frames(states), actions, rewards, scale_frames(next_states), dones\n",
        "  \n",
        "  @abstractmethod\n",
        "  def sample_batch(self, batch_size):\n",
        "    pass\n",
        "\n",
        "class AbstractReplayMemory(ReplayMemory):\n",
        "  def __init__(self, max_size):\n",
        "    self._buffer = deque(maxlen = max_size)\n",
        "\n",
        "  def len(self):\n",
        "    return len(self._buffer)\n",
        "\n",
        "  @abstractmethod\n",
        "  def add_transition(self, transition):\n",
        "    pass\n",
        "  \n",
        "  @abstractmethod\n",
        "  def sample_batch(self, batch_size):\n",
        "    pass\n",
        "\n",
        "class SimpleReplayMemory(AbstractReplayMemory):\n",
        "  def __init__(self, max_size):\n",
        "     super().__init__(max_size)\n",
        "\n",
        "  def add_transition(self, transition):\n",
        "    self._buffer.append(transition)\n",
        "    return self\n",
        "\n",
        "  def sample_batch(self, batch_size):\n",
        "    indices = np.random.randint(len(self._buffer), size=batch_size)\n",
        "    batch = [self._buffer[index] for index in indices]\n",
        "    return self._split_batch(batch)\n",
        "\n",
        "\n",
        "class PrioritizedReplayMemory(ReplayMemory):\n",
        "  @abstractmethod\n",
        "  def update_priorities(self, indices, losses):\n",
        "    pass\n",
        "\n",
        "\n",
        "class PrioritizedReplayMemoryImpl(AbstractReplayMemory, PrioritizedReplayMemory):\n",
        "  def __init__(self, max_size, alpha = 0.6, error_offset = 0.1):\n",
        "    super().__init__(max_size)\n",
        "    self._alpha = alpha\n",
        "    self._priorities = deque(maxlen = max_size)\n",
        "    self._default_max_priority = 1\n",
        "    self._error_offset = 0.1\n",
        "\n",
        "  def _get_probabilities(self):\n",
        "    scaled_priorities = np.array(self._priorities) ** self._alpha # pi^a\n",
        "    sample_probabilities = scaled_priorities/ np.sum(scaled_priorities) # P(i) = pi^a/sum(p^a)\n",
        "    return sample_probabilities\n",
        "\n",
        "  def _get_importance(self, probabilities, beta):\n",
        "    importance = (1/len(self._buffer) * 1/probabilities) ** beta # (1/N * 1/P(i))^b\n",
        "    normalized_importance = importance / max(importance) # 1/maxi(wi), normalization\n",
        "    return normalized_importance\n",
        "\n",
        "  def update_priorities(self, indices, losses):\n",
        "    for i,l in zip(indices, losses):\n",
        "      self._priorities[i] = max(abs(l) + self._error_offset, self._default_max_priority)\n",
        "    \n",
        "    return self\n",
        "\n",
        "  def add_transition(self, transaction):\n",
        "    self._buffer.append(transaction)\n",
        "    self._priorities.append(max(self._priorities, default = self._default_max_priority))\n",
        "\n",
        "  def sample_batch(self, batch_size, beta):\n",
        "    probs = self._get_probabilities()\n",
        "    sampled_indices = random.choices(range(len(self._buffer)), k=batch_size, weights = probs)\n",
        "    return (*self._split_batch(np.array(self._buffer)[sampled_indices]), self._get_importance(probs[sampled_indices], beta), sampled_indices)\n",
        "\n",
        "\n",
        "class TreePrioritizedReplayMemory(PrioritizedReplayMemory):\n",
        "  def __init__(self, capacity, alpha = 0.6, error_offset = 0.1):\n",
        "    self._capacity = capacity\n",
        "    self._alpha = alpha\n",
        "    self._priority_sum = np.zeros(shape= 2 * capacity, dtype=np.float32)\n",
        "    self._priority_min = [float('inf') for _ in range( 2 * self._capacity)]\n",
        "    self._max_priority = 1\n",
        "    self._data = [None for _ in range(self._capacity)]\n",
        "    self._next_idx = 0\n",
        "    self._size = 0\n",
        "    self._error_offset = error_offset\n",
        "\n",
        "  def len(self):\n",
        "    return self._size\n",
        "\n",
        "  def add_transition(self, transaction):\n",
        "    idx = self._next_idx\n",
        "    self._data[idx] = transaction\n",
        "    self._next_idx = (idx + 1) % self._capacity\n",
        "    self._size = min(self._capacity, self._size + 1)\n",
        "\n",
        "    priority_alpha = self._max_priority ** self._alpha\n",
        "    self._set_priority_min(idx, priority_alpha)\n",
        "    self._set_priority_sum(idx, priority_alpha)\n",
        "\n",
        "  def _set_priority_min(self, idx, priority_alpha):\n",
        "    idx += self._capacity\n",
        "    self._priority_min[idx] = priority_alpha\n",
        "\n",
        "    while idx >= 2:\n",
        "      idx //= 2\n",
        "      self._priority_min[idx] = min(self._priority_min[2 * idx], self._priority_min[2 * idx + 1])\n",
        "\n",
        "  def _set_priority_sum(self, idx, priority):\n",
        "    idx += self._capacity\n",
        "    self._priority_sum[idx] = priority\n",
        "    while idx >= 2:\n",
        "      idx //= 2\n",
        "      self._priority_sum[idx] = self._priority_sum[2 * idx] + self._priority_sum[2 * idx + 1]\n",
        "\n",
        "  def _sum(self):\n",
        "    return self._priority_sum[1]\n",
        "\n",
        "  def _min(self):\n",
        "    return self._priority_min[1]\n",
        "\n",
        "  def _find_prefix_sum_idx(self, prefix_sum):\n",
        "    idx = 1\n",
        "    while idx < self._capacity:\n",
        "        if self._priority_sum[idx * 2] > prefix_sum:\n",
        "          idx = 2 * idx\n",
        "        else:\n",
        "          prefix_sum -= self._priority_sum[idx * 2]\n",
        "          idx = 2 * idx + 1\n",
        "    return idx - self._capacity\n",
        "\n",
        "  def sample_batch(self, batch_size, beta):\n",
        "    weights = np.zeros(shape=batch_size, dtype=np.float32)\n",
        "    indices = np.zeros(shape=batch_size, dtype=np.int32)\n",
        "\n",
        "    # Get sample indices\n",
        "    for i in range(batch_size):\n",
        "      p = random.random() * self._sum()\n",
        "      indices[i] = self._find_prefix_sum_idx(p)\n",
        "\n",
        "    prob_min = self._min() / self._sum()\n",
        "    max_weight = (prob_min * self._size) ** (-beta)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      idx = indices[i]\n",
        "      prob = self._priority_sum[idx + self._capacity] / self._sum()\n",
        "      weight = (prob * self._size) ** (-beta)\n",
        "      weights[i] = weight / max_weight\n",
        "\n",
        "    return (*self._split_batch(self._data[:self._size]), weights, indices)\n",
        "\n",
        "  def update_priorities(self, indices, losses):\n",
        "    priorities = np.abs(losses + self._error_offset)\n",
        "\n",
        "    for idx, priority in zip(indices, priorities):\n",
        "      self._max_priority = max(self._max_priority, priority)\n",
        "      priority_alpha = priority ** self._alpha\n",
        "      self._set_priority_min(idx, priority_alpha)\n",
        "      self._set_priority_sum(idx, priority_alpha)\n",
        "\n",
        "  def is_full(self):\n",
        "    return self._capacity == self._size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TreePrioritizedReplayMemory(PrioritizedReplayMemory):\n",
        "  def __init__(self, capacity, alpha=0.6, error_offset=0.1):\n",
        "    # We use a power of $2$ for capacity because it simplifies the code and debugging\n",
        "    self._capacity = capacity\n",
        "    # $\\alpha$\n",
        "    self._alpha = alpha\n",
        "\n",
        "    self._error_offset = error_offset\n",
        "\n",
        "    # Maintain segment binary trees to take sum and find minimum over a range\n",
        "    self._priority_sum = [0 for _ in range(2 * self._capacity)]\n",
        "    self._priority_min = np.repeat([float('inf')], 2 * self._capacity)\n",
        "    self._data = np.zeros(capacity, dtype=object)\n",
        "    # Current max priority, $p$, to be assigned to new transitions\n",
        "    self._max_priority = 1.\n",
        "\n",
        "    # We use cyclic buffers to store data, and `next_idx` keeps the index of the next empty\n",
        "    # slot\n",
        "    self._next_idx = 0\n",
        "\n",
        "    # Size of the buffer\n",
        "    self._size = 0\n",
        "\n",
        "  def len(self):\n",
        "    return self._size\n",
        "\n",
        "  def add_transition(self, transition):\n",
        "    # Get next available slot\n",
        "    idx = self._next_idx\n",
        "\n",
        "    # store in the queue\n",
        "    self._data[idx] = transition\n",
        "\n",
        "    # Increment next available slot\n",
        "    self._next_idx = (idx + 1) % self._capacity\n",
        "    # Calculate the size\n",
        "    self._size = min(self._capacity, self._size + 1)\n",
        "\n",
        "    # $p_i^\\alpha$, new samples get `max_priority`\n",
        "    priority_alpha = self._max_priority ** self._alpha\n",
        "    # Update the two segment trees for sum and minimum\n",
        "    self._set_priority_min(idx, priority_alpha)\n",
        "    self._set_priority_sum(idx, priority_alpha)\n",
        "\n",
        "  def _set_priority_min(self, idx, priority_alpha):\n",
        "    # Leaf of the binary tree\n",
        "    idx += self._capacity\n",
        "    self._priority_min[idx] = priority_alpha\n",
        "\n",
        "    # Update tree, by traversing along ancestors.\n",
        "    # Continue until the root of the tree.\n",
        "    while idx >= 2:\n",
        "      # Get the index of the parent node\n",
        "      idx //= 2\n",
        "      # Value of the parent node is the minimum of it's two children\n",
        "      self._priority_min[idx] = min(self._priority_min[2 * idx], self._priority_min[2 * idx + 1])\n",
        "\n",
        "  def _set_priority_sum(self, idx, priority):\n",
        "    # Leaf of the binary tree\n",
        "    idx += self._capacity\n",
        "    # Set the priority at the leaf\n",
        "    self._priority_sum[idx] = priority\n",
        "\n",
        "    # Update tree, by traversing along ancestors.\n",
        "    # Continue until the root of the tree.\n",
        "    while idx >= 2:\n",
        "      # Get the index of the parent node\n",
        "      idx //= 2\n",
        "      # Value of the parent node is the sum of it's two children\n",
        "      self._priority_sum[idx] = self._priority_sum[2 * idx] + self._priority_sum[2 * idx + 1]\n",
        "\n",
        "  def _sum(self):\n",
        "    # The root node keeps the sum of all values\n",
        "    return self._priority_sum[1]\n",
        "\n",
        "  def _min(self):\n",
        "    # The root node keeps the minimum of all values\n",
        "    return self._priority_min[1]\n",
        "\n",
        "  def _find_prefix_sum_idx(self, prefix_sum):\n",
        "    # Start from the root\n",
        "    idx = 1\n",
        "    while idx < self._capacity:\n",
        "      # If the sum of the left branch is higher than required sum\n",
        "      if self._priority_sum[idx * 2] > prefix_sum:\n",
        "        # Go to left branch of the tree\n",
        "        idx = 2 * idx\n",
        "      else:\n",
        "        # Otherwise go to right branch and reduce the sum of left\n",
        "        #  branch from required sum\n",
        "        prefix_sum -= self._priority_sum[idx * 2]\n",
        "        idx = 2 * idx + 1\n",
        "\n",
        "    # We are at the leaf node. Subtract the capacity by the index in the tree\n",
        "    # to get the index of actual value\n",
        "    return idx - self._capacity\n",
        "\n",
        "  def sample_batch(self, batch_size, beta):\n",
        "    # Initialize samples\n",
        "    weights = np.zeros(shape=batch_size, dtype=np.float32)\n",
        "    indices = np.zeros(shape=batch_size, dtype=np.int32)\n",
        "\n",
        "    # Get sample indices\n",
        "    for i in range(batch_size):\n",
        "      p = random.random() * self._sum()\n",
        "      idx = self._find_prefix_sum_idx(p)\n",
        "      indices[i] = idx\n",
        "\n",
        "    # $\\min_i P(i) = \\frac{\\min_i p_i^\\alpha}{\\sum_k p_k^\\alpha}$\n",
        "    prob_min = self._min() / self._sum()\n",
        "    # $\\max_i w_i = \\bigg(\\frac{1}{N} \\frac{1}{\\min_i P(i)}\\bigg)^\\beta$\n",
        "    max_weight = (prob_min * self._size) ** (-beta)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      idx = indices[i]\n",
        "      # $P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$\n",
        "      prob = self._priority_sum[idx + self._capacity] / self._sum()\n",
        "      # $w_i = \\bigg(\\frac{1}{N} \\frac{1}{P(i)}\\bigg)^\\beta$\n",
        "      weight = (prob * self._size) ** (-beta)\n",
        "      # Normalize by $\\frac{1}{\\max_i w_i}$,\n",
        "      #  which also cancels off the $\\frac{1}{N}$ term\n",
        "      weights[i] = weight / max_weight\n",
        "\n",
        "    return (*self._split_batch(self._data[indices]), weights, indices)\n",
        "\n",
        "  def update_priorities(self, indexes, losses):\n",
        "    priorities = np.abs(losses + self._error_offset)\n",
        "    for idx, priority in zip(indexes, priorities):\n",
        "      # Set current max priority\n",
        "      self._max_priority = max(self._max_priority, priority)\n",
        "\n",
        "      # Calculate $p_i^\\alpha$\n",
        "      priority_alpha = priority ** self._alpha\n",
        "      # Update the trees\n",
        "      self._set_priority_min(idx, priority_alpha)\n",
        "      self._set_priority_sum(idx, priority_alpha)"
      ],
      "metadata": {
        "id": "oFYKFOXlDr9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bui9SOmyFIGl"
      },
      "outputs": [],
      "source": [
        "def dqn_replay_memory_init(env,\n",
        "                           replay_memory,\n",
        "                           replay_memory_init_size,\n",
        "                           episode_max_steps,\n",
        "                           preprocess_frame,\n",
        "                           stacked_frame_count):\n",
        "  loading_step = replay_memory_init_size/10\n",
        "\n",
        "  while True:\n",
        "    initial_state = env.reset()\n",
        "    frames_stack = FramesStack(initial_state, stacked_frame_count, preprocess_frame)\n",
        "    stacked_states = frames_stack.stack()\n",
        "\n",
        "    done=False\n",
        "    step_count=0\n",
        "\n",
        "    while step_count < episode_max_steps and not done:\n",
        "      if replay_memory.len() % loading_step == 0:\n",
        "        print(f'Loaded: {int(replay_memory.len()/replay_memory_init_size * 100)}% ({replay_memory.len()})')\n",
        "\n",
        "      action = env.action_space.sample()\n",
        "      new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      frames_stack.add(new_state)\n",
        "      new_stacked_states = frames_stack.stack()\n",
        "\n",
        "      replay_memory.add_transition([stacked_states, action, reward, new_stacked_states, done])\n",
        "      stacked_states = new_stacked_states\n",
        "      step_count+=1\n",
        "      \n",
        "\n",
        "      if replay_memory.len() >= replay_memory_init_size:\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CbsWHS7pO0t"
      },
      "source": [
        "## Epsilon update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mdHND0KpOgV"
      },
      "outputs": [],
      "source": [
        "def update_epsilon(epsilon, min_epsilon, decay):\n",
        "  return max(min_epsilon,epsilon-decay)\n",
        "\n",
        "def update_beta(beta, max_beta, beta_increase):\n",
        "  return min(max_beta, beta+beta_increase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InOQ1MBN0aef"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To select an action using this DQN, we just pick the action with the largest predicted Q-value. However, to ensure that the agent explores the environment, we choose a random action with probability epsilon."
      ],
      "metadata": {
        "id": "70kovF7tGZTX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACkgPygKNd6r"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(action_model, state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        Q_values = action_model.predict(state[np.newaxis], verbose=False)\n",
        "        return Q_values.argmax()  # optimal action according to the DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop of the game: the agent chooses an action, performs it and the state is updated."
      ],
      "metadata": {
        "id": "4UCcTe-0Ghi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjfT8qFw0aJ-"
      },
      "outputs": [],
      "source": [
        "def game_loop(env, action_model, states, epsilon):\n",
        "  float_states = scale_frames(states)\n",
        "  action = epsilon_greedy_policy(action_model, float_states, epsilon)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  return action, reward, next_state, done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a transition < 𝑠𝑡\n",
        ", 𝑎𝑡\n",
        ", 𝑟𝑡+1, 𝑠𝑡+1 >, 𝑄 𝑠𝑡\n",
        ", 𝑎𝑡 can be expressed by the Bellman\n",
        "equation in terms of Q-value of next state 𝑠𝑡+1:\n",
        "𝑄 𝑠𝑡\n",
        ", 𝑎𝑡 = 𝑟𝑡+1 + 𝛾 ⋅ max\n",
        "𝑎\n",
        "𝑄 𝑠𝑡+1.\n",
        "\n",
        "The maximum future reward for current state 𝑠𝑡 and action 𝑎𝑡\n",
        "is the immediate reward\n",
        "𝑟𝑡+1 plus maximum future reward for the next state 𝑠𝑡+1.\n",
        " In other words, instead of calculating each value as the sum of the expected cumulative\n",
        "rewards (which is a long process), this is equivalent to sum the immediate reward and the\n",
        "discounted future reward of the state that follows.\n",
        "\n",
        "\n",
        "\n",
        "Since the true value is unknown (RL is unsupervised, and no labels are available), it will be estimated using the Bellman equation:\n",
        "𝑦𝑡 = 𝑟𝑡+1 + 𝛾 ⋅ max 𝑄 (a) (𝑠𝑡+1,a)\n"
      ],
      "metadata": {
        "id": "cP6q0k6cHpZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target Q values computation\n",
        "def compute_Q_values(max_next_Q_values, reward_batch, done_batch, gamma):\n",
        "  target_Q_values = reward_batch + (1 - done_batch) * gamma * max_next_Q_values\n",
        "  target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "  return target_Q_values\n",
        "\n",
        "def target_Q_values(target_model, new_state_batch, reward_batch, done_batch, gamma):\n",
        "  next_Q_values = target_model.predict(new_state_batch, verbose=0)\n",
        "  max_next_Q_values = next_Q_values.max(axis=1)\n",
        "  target_Q_values = compute_Q_values(max_next_Q_values, reward_batch, done_batch, gamma)\n",
        "  return max_next_Q_values\n",
        "\n",
        "def double_DQN_values(action_model, target_model, new_state_batch, reward_batch, done_batch, gamma, n_actions):\n",
        "  next_Q_values = action_model.predict(new_state_batch, verbose=0)  # ≠ target.predict()\n",
        "  best_next_actions = next_Q_values.argmax(axis=1)\n",
        "  next_mask = tf.one_hot(best_next_actions, n_actions).numpy()\n",
        "  max_next_Q_values = (target_model.predict(new_state_batch, verbose=0) * next_mask).sum(axis=1)\n",
        "  target_Q_values = compute_Q_values(max_next_Q_values, reward_batch, done_batch, gamma)\n",
        "  return target_Q_values"
      ],
      "metadata": {
        "id": "2BgWce932ngj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-values can be any real values, which makes it a regression task, that can be optimized\n",
        "with a simple square error loss.\n",
        "\n",
        "Perform a gradient descent step to update 𝑄 weights by minimizing the loss function.\n",
        "\n",
        "To solve the moving target problem, two models (action and target) are used during the DQN training process."
      ],
      "metadata": {
        "id": "kD3ZpNaXOWig"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMnsTDC0E2J"
      },
      "source": [
        "### **Action model update**\n",
        "The following function updates action model weights using *gradient descent* algorithm given:\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- a mini-batch containing transitions $<s_i,a_i,r_{i+1},s_{i+1}>$ randomly selected from the replay memory (*mini_batch*); \n",
        "- the discount factor $\\gamma$ (gamma).\n",
        "\n",
        "With respect to **simple_dqn_update** function, here the pixel values contained in $s_i$ and $s_{i+1}$ (*state_batch* and *new_state_batch*, respectively) are normalized in the range $[0;1]$ before they being used as input of **predict** and **train_on_batch** methods."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Double DQN:**\n",
        "\n",
        "One of the problems of the DQN algorithm is that it overestimates the true rewards; the Q-values think the agent is going to obtain a higher return than what it will obtain in reality. To fix this, the authors of the Double DQN algorithm [1] suggest using a simple trick: decoupling the action selection from the action evaluation. Instead of using the same Bellman equation as in the DQN algorithm, they change it like this:\n",
        "\n",
        "<img src=https://miro.medium.com/max/720/1*OFYk-9yL6k8QiIUYjjTUXQ.png width=\"500\" height= \"50\">\n",
        "\n",
        "First, the main neural network θ decides which one is the best next action a’ among all the available next actions, and then the target neural network evaluates this action to know its Q-value. This simple trick has shown to reduce overestimations, which results in better final policies."
      ],
      "metadata": {
        "id": "eBRI_tPKaS59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generic model update\n",
        "def update_model(action_model, target_Q_values, state_batch, action_batch, optimizer, loss_fn, n_actions):\n",
        "  mask = tf.one_hot(action_batch, n_actions)\n",
        "  with tf.GradientTape() as tape:\n",
        "    all_Q_values = action_model(state_batch)\n",
        "    Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "    losses = loss_fn(target_Q_values, Q_values)\n",
        "    loss = tf.reduce_mean(losses)\n",
        "\n",
        "  grads = tape.gradient(loss, action_model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, action_model.trainable_variables))\n",
        "  return losses\n",
        "\n",
        "def prioritized_model_update(action_model, target_model, target_Q_values_fn, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  state_batch, action_batch, reward_batch, new_state_batch, done_batch, importances, indices = batch\n",
        "  target_Q_values = target_Q_values_fn(action_model, target_model, new_state_batch, reward_batch, done_batch, gamma, n_actions)\n",
        "  new_loss_fn = lambda target_values, predicted_values: loss_fn(target_values, predicted_values) * importances\n",
        "  losses = update_model(action_model, target_Q_values, state_batch, action_batch, optimizer, new_loss_fn, n_actions)\n",
        "  return losses, indices\n",
        "\n",
        "def simple_model_update(action_model, target_model, target_Q_values_fn, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  state_batch, action_batch, reward_batch, new_state_batch, done_batch = batch\n",
        "  target_Q_values = target_Q_values_fn(action_model, target_model, new_state_batch, reward_batch, done_batch, gamma, n_actions)\n",
        "  update_model(action_model, target_Q_values, state_batch, action_batch, optimizer, loss_fn, n_actions)"
      ],
      "metadata": {
        "id": "_hnpnm8a35Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double dqn\n",
        "def double_dqn_prioritized_update(action_model, target_model, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  return prioritized_model_update(action_model, target_model, double_DQN_values, batch, gamma, optimizer, loss_fn, n_actions)\n",
        "\n",
        "def double_dqn_update(action_model, target_model, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  return simple_model_update(action_model, target_model, double_DQN_values, batch, gamma, optimizer, loss_fn, n_actions)\n",
        "\n",
        "# adapter\n",
        "def target_Q_value_adapter(action_model, target_model, new_state_batch, reward_batch, done_batch, gamma, n_actions):\n",
        "  return target_Q_values(target_model, new_state_batch, reward_batch, done_batch, gamma)\n",
        "\n",
        "# Fixed Q\n",
        "def fixed_Q_prioritized_update(action_model, target_model, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  return prioritized_model_update(action_model, target_model, target_Q_value_adapter, batch, gamma, optimizer, loss_fn, n_actions)\n",
        "\n",
        "def fixed_Q_update(action_model, target_model, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  return simple_model_update(action_model, target_model, target_Q_value_adapter, batch, gamma, optimizer, loss_fn, n_actions)\n",
        "\n",
        "# DQN\n",
        "def Q_update_prioritized(action_model, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  return prioritized_model_update(action_model, action_model, target_Q_value_adapter, batch, gamma, optimizer, loss_fn, n_actions)\n",
        "\n",
        "def Q_update(action_model, batch, gamma, optimizer, loss_fn, n_actions):\n",
        "  return simple_model_update(action_model, action_model, target_Q_value_adapter, batch, gamma, optimizer, loss_fn, n_actions)"
      ],
      "metadata": {
        "id": "iFyGvn5T25CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, \n",
        "          loss_fn, \n",
        "          optimizer,\n",
        "          mode='DQN', \n",
        "          dueling=False,\n",
        "          episode_count=10_000,\n",
        "          episode_max_steps=1000,\n",
        "          replay_memory_max_size=1_000_000,\n",
        "          replay_memory_init_size=50_000,\n",
        "          batch_size=32,\n",
        "          step_per_update=4,\n",
        "          step_per_update_target_model=500,\n",
        "          max_epsilon=1,\n",
        "          min_epsilon=0.1,\n",
        "          epsilon_decay=1e-06,\n",
        "          gamma=0.99,\n",
        "          prioritized_exp_replay=False,\n",
        "          alpha=0.6,\n",
        "          max_beta=1,\n",
        "          min_beta=0.4,\n",
        "          beta_increase=1e-06,\n",
        "          preprocess_frame_fn=lambda x:x,\n",
        "          stacked_frame_count=4,\n",
        "          moving_avg_window_size = 100,\n",
        "          moving_avg_stop_thr = None,\n",
        "          prev_rewards = [],\n",
        "          episodes_for_saving=20,\n",
        "          save_data_to=None,\n",
        "          read_data_from=None):\n",
        "\n",
        "  #----------------- GLOBAL DECLARATIONS -----------------\n",
        "  n_actions = env.action_space.n\n",
        "  epsilon = max_epsilon\n",
        "  beta = min_beta\n",
        "  train_rewards = prev_rewards.copy()\n",
        "  train_step_count = 0 #T\n",
        "  to_save = {}\n",
        "\n",
        "  # define the type of replay memory\n",
        "  if prioritized_exp_replay == True:\n",
        "    replay_memory = PrioritizedReplayMemoryImpl(replay_memory_max_size, alpha)\n",
        "  elif prioritized_exp_replay == 'Tree':\n",
        "    replay_memory = TreePrioritizedReplayMemory(replay_memory_max_size, alpha)\n",
        "  else:\n",
        "    replay_memory = SimpleReplayMemory(replay_memory_max_size)\n",
        "\n",
        "  # define nets\n",
        "  env_shape = preprocess_frame_fn(env.reset()).shape\n",
        " \n",
        "\n",
        "  # define model structure\n",
        "  if mode == 'DoubleDQN' or mode == 'FixedDQN':\n",
        "    if dueling:\n",
        "      action_model, target_model = build_dueling_action_target_dqn((*env_shape, stacked_frame_count), n_actions)\n",
        "    else:\n",
        "      action_model, target_model = build_action_target_dqn((*env_shape, stacked_frame_count), n_actions)\n",
        "  else: # DQN\n",
        "    if dueling:\n",
        "      action_model = build_dueling_dqn((*env_shape, stacked_frame_count), n_actions)\n",
        "    else:\n",
        "      action_model = build_dqn((*env_shape, stacked_frame_count), n_actions)\n",
        "\n",
        "  # define update function\n",
        "  if mode == 'DoubleDQN':\n",
        "    if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "      model_update_fn = double_dqn_prioritized_update\n",
        "    else:\n",
        "      model_update_fn = double_dqn_update\n",
        "  elif mode == 'FixedDQN':\n",
        "    if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "      model_update_fn = fixed_Q_prioritized_update\n",
        "    else:\n",
        "      mode_update_fn = fixed_Q_update\n",
        "  else: # DQN\n",
        "    if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "      model_update_fn = Q_update_prioritized\n",
        "    else:\n",
        "      model_update_fn = Q_update\n",
        "\n",
        "  # ----------------- READ DATA FROM FILE -----------------\n",
        "  if read_data_from is not None:\n",
        "    prev_data = load_data(read_data_from)\n",
        "    if prev_data:\n",
        "      train_rewards = prev_data.get('rewards', train_rewards)\n",
        "      action_model = prev_data.get('action_model', action_model)\n",
        "      epsilon = prev_data.get('epsilon', epsilon)\n",
        "\n",
        "      if mode == 'DoubleDQN' or mode == 'FixedDQN':\n",
        "        target_model = prev_data.get('target_model', target_model)\n",
        "        \n",
        "      if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "        beta = prev_data.get('beta', min_beta)\n",
        "      print(f'Training data restored from file: {read_data_from}')\n",
        "\n",
        "  # ----------------- INITIALIZATIONS -----------------\n",
        "  # fill replay memory\n",
        "  print('Initializing replay memory')\n",
        "  dqn_replay_memory_init(env, \n",
        "                         replay_memory, \n",
        "                         replay_memory_init_size, \n",
        "                         episode_max_steps, \n",
        "                         preprocess_frame_fn, \n",
        "                         stacked_frame_count)\n",
        "  \n",
        "\n",
        "  # ----------------- START TIMER -----------------\n",
        "  train_start_time = time.time()\n",
        "\n",
        "  # ----------------- TRAINING -----------------\n",
        "  print(\"training started\")\n",
        "  for n in range(episode_count): \n",
        "    #initialization\n",
        "    # save initial rewards and epsilon (visualization)\n",
        "    episode_reward = 0\n",
        "    episode_start_epsilon = epsilon\n",
        "    # reset the environment and preprocess initial state\n",
        "    starting_state = env.reset()\n",
        "    # initialize FramesStack\n",
        "    frames_deque = FramesStack(starting_state, stacked_frame_count, preprocess_frame_fn)\n",
        "    stacked_states = frames_deque.stack()\n",
        "    step_count = 0  #t\n",
        "    done = False\n",
        "    \n",
        "    while step_count < episode_max_steps and not done: \n",
        "      # perform a game loop\n",
        "      action, reward, next_state, done = game_loop(env, action_model, stacked_states, epsilon)\n",
        "        \n",
        "      # the stack of frames is updated by adding the new frame and removing the oldest one\n",
        "      frames_deque.add(next_state)\n",
        "      new_stacked_states = frames_deque.stack()\n",
        "      # store transition in the replay memory\n",
        "      replay_memory.add_transition([stacked_states, action, reward, new_stacked_states, done])\n",
        "      \n",
        "      # update the current state\n",
        "      stacked_states = new_stacked_states\n",
        "\n",
        "      if train_step_count % step_per_update == 0 and replay_memory.len() >= batch_size:\n",
        "        # get a random mini-batch from the replay memory\n",
        "        if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "          mini_batch = replay_memory.sample_batch(batch_size, beta)\n",
        "        else:\n",
        "          mini_batch = replay_memory.sample_batch(batch_size)\n",
        "\n",
        "        # update model\n",
        "        if mode == 'DoubleDQN' or mode == 'FixedDQN':\n",
        "          if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "            losses, indices = model_update_fn(action_model, \n",
        "                                              target_model, \n",
        "                                              mini_batch, \n",
        "                                              gamma, \n",
        "                                              optimizer, \n",
        "                                              loss_fn,\n",
        "                                              n_actions)\n",
        "          else:\n",
        "            model_update_fn(action_model, target_model, mini_batch, gamma, optimizer, loss_fn, n_actions)\n",
        "        else: # DQN\n",
        "          if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "            losses, indices = model_update_fn(action_model, \n",
        "                                              mini_batch, \n",
        "                                              gamma, \n",
        "                                              optimizer, \n",
        "                                              loss_fn,\n",
        "                                              n_actions)\n",
        "          else:\n",
        "            model_update_fn(action_model, mini_batch, gamma, optimizer, loss_fn, n_actions)\n",
        "\n",
        "        # update priorities in replay memory \n",
        "        if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree': \n",
        "          replay_memory.update_priorities(indices, losses)\n",
        "\n",
        "          \n",
        "      if train_step_count % step_per_update_target_model == 0 and (mode == 'DoubleDQN' or mode == 'FixedDQN'):\n",
        "        # copy weights from action to target model\n",
        "        target_model.set_weights(action_model.get_weights())\n",
        "\n",
        "      # reduce epsilon\n",
        "      epsilon = update_epsilon(epsilon, min_epsilon, epsilon_decay)\n",
        "      beta = update_beta(beta, max_beta, beta_increase)\n",
        "      \n",
        "      # increase episode step count and total step count\n",
        "      step_count += 1\n",
        "      train_step_count += 1\n",
        "\n",
        "      # add the current reward to the episode total reward\n",
        "      episode_reward += reward \n",
        "    \n",
        "    # put the episode total reward into a list for visualization purposes\n",
        "    train_rewards.append(episode_reward)\n",
        "\n",
        "    if n > 0 and n % episodes_for_saving == 0 and save_data_to is not None:\n",
        "      # save model \n",
        "      to_save['rewards'] = train_rewards\n",
        "      to_save['action_model'] = action_model\n",
        "      to_save['epsilon'] = epsilon\n",
        "\n",
        "      # add target model\n",
        "      if mode == 'DoubleDQN' or mode == 'FixedDQN':\n",
        "        to_save['target_model'] = target_model\n",
        "      \n",
        "      if prioritized_exp_replay == True or prioritized_exp_replay == 'Tree':\n",
        "        to_save['beta'] = beta\n",
        "\n",
        "      write_data(to_save, save_data_to)\n",
        "      print('Data saved')\n",
        "\n",
        "    # for visualization purposes\n",
        "    \n",
        "    moving_avg_reward = statistics.mean(train_rewards[-moving_avg_window_size:])\n",
        "    print(f'Episode: {n+1}° total reward: {episode_reward} [{moving_avg_reward:.2f}]')\n",
        "    \n",
        "    # condition to consider the task solved\n",
        "    if (moving_avg_stop_thr is not None) and moving_avg_reward > moving_avg_stop_thr:\n",
        "      break\n",
        "\n",
        "  # --------------- COMPLETE TIME PRINT ---------------\n",
        "  print(f'Train time: {time.time() - train_start_time}s]')\n",
        "  # return a list containing the total rewards of all training episodes\n",
        "  return train_rewards"
      ],
      "metadata": {
        "id": "rr0HIYeh3lxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters"
      ],
      "metadata": {
        "id": "pxCvl2t3mAnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_count = 10000                  # Total number of training episodes\n",
        "episode_max_steps= 1000            # Maximum number of steps per episode\n",
        "\n",
        "replay_memory_max_size = 1_000_000  # The maximum number of transitions stored into the replay memory. The Deepmind paper suggests 1M however this may cause memory issues.\n",
        "replay_memory_init_size= 50_000       # The maximum number of transitions stored into the replay memory. The Deepmind paper suggests 50K.\n",
        "batch_size = 32                     # The mini-batch size\n",
        "\n",
        "step_per_update = 4                 # The number of total steps executed between successive updates of the action model weights\n",
        "step_per_update_target_model = 10_000  # The number of total steps executed between successive replaces of the target model weights\n",
        "\n",
        "max_epsilon=1.0                     # Exploration probability at start\n",
        "min_epsilon=0.1                     # Minimum exploration probability\n",
        "epsilon_decay=(max_epsilon-min_epsilon) / 1_000_000.0  # Decay for exploration probability\n",
        "\n",
        "gamma = 0.99                        # Discount factor\n",
        "\n",
        "stacked_frame_count= 4               # number of stacked frames\n",
        "\n",
        "moving_avg_window_size = 100          # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
        "moving_avg_stop_thr = 60              # Minimum value of the total reward moving average to consider the task solved\n",
        "\n",
        "\n",
        "learning_rate = 0.00025\n",
        "momentum = 0.95\n",
        "\n",
        "#prioritized exp replay \n",
        "min_beta = 0.4 \n",
        "max_beta = 1\n",
        "beta_increase = (max_beta-min_beta) / 1_000_000.0\n",
        "\n",
        "env = preprocessed_env()\n",
        "#env = make_env()\n",
        "loss_fn = Huber()\n",
        "#optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "optimizer = RMSprop(learning_rate = learning_rate, momentum = momentum)"
      ],
      "metadata": {
        "id": "xt4YGNe8StMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN"
      ],
      "metadata": {
        "id": "84Q5S9cwmu5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(env, \n",
        "      loss_fn,\n",
        "      optimizer, \n",
        "      dueling=False,\n",
        "      prioritized_exp_replay='Tree',\n",
        "      epsilon_decay=epsilon_decay,\n",
        "      beta_increase=beta_increase,\n",
        "      replay_memory_init_size=1000,\n",
        "      mode='DQN', \n",
        "      save_data_to=data_path,\n",
        "      read_data_from=data_path,\n",
        "      preprocess_frame_fn=lambda x : x.squeeze())"
      ],
      "metadata": {
        "id": "92riRSW7k3AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK2n5aJEIYsL"
      },
      "source": [
        "# StableBaselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYdRlCdp7-gw"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "\n",
        "def make_stable_baselines_env():\n",
        "  N_ENVS = 1\n",
        "  FRAMED_STACKED = 4\n",
        "  SEED = 1234\n",
        "  env = make_atari_env(ENV_NAME, n_envs=N_ENVS, seed=SEED)\n",
        "  #env = VecFrameStack(env, n_stack=FRAMED_STACKED)\n",
        "  return env\n",
        "\n",
        "def play_env_stable_baselines(model, steps):\n",
        "  model_name = type(model).__name__\n",
        "  env = VecVideoRecorder(make_stable_baselines_env(),\n",
        "                         model_name,\n",
        "                         record_video_trigger=lambda x: x == 0, \n",
        "                         video_length=steps,\n",
        "                         name_prefix=f\"{model_name}\")\n",
        "  observations = env.reset()\n",
        "  \n",
        "  for _ in range(steps):\n",
        "    actions, _ = model.predict(observations)\n",
        "    observations, rewards, terminateds, _ = env.step(actions)\n",
        "    if all(terminateds):\n",
        "      break\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCkm0Z11VEcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f4a948-9bbb-4527-bd9d-7f8b45a2e5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "TRAIN_STEPS = 1000\n",
        "env = make_stable_baselines_env()\n",
        "\n",
        "def train_model_save_video(model, steps_for_video):\n",
        "  %tensorboard --logdir log --reload_multifile True\n",
        "  model.learn(total_timesteps=TRAIN_STEPS)\n",
        "  model.save(\"{}_model\".format(type(model).__name__))\n",
        "  play_env_stable_baselines(model, steps_for_video)\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttTtfsi_hFVE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "ZPl4_mvJuetl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esToJQxWuetl"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.atari_wrappers import AtariWrapper, EpisodicLifeEnv, WarpFrame, MaxAndSkipEnv\n",
        "from stable_baselines3.a2c import A2C\n",
        "from stable_baselines3.ppo import PPO\n",
        "from stable_baselines3.common.sb2_compat.rmsprop_tf_like import RMSpropTFLike\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack, DummyVecEnv, VecMonitor\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GLOBAL VARIABLES"
      ],
      "metadata": {
        "id": "wl_-5JPPPuai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 120_000_000\n",
        "frame_stack = 4\n",
        "frame_skip = 4\n",
        "ENV_NAME = \"ALE/Pong-v5\""
      ],
      "metadata": {
        "id": "zxS2V-id0ujT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UTILITY FUNCTIONS"
      ],
      "metadata": {
        "id": "Vm9rmZxPPw0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_base_env(env_name, frame_skip=4):\n",
        "  base_env = gym.make(env_name)\n",
        "  life_env = EpisodicLifeEnv(base_env)\n",
        "  warp_env = WarpFrame(life_env)\n",
        "  skip_env = MaxAndSkipEnv(warp_env, frame_skip)\n",
        "  return skip_env\n",
        "\n",
        "def make_vec_env(env_name, n_envs=4, frame_stack=4, frame_skip=4, monitor_log=None):\n",
        "  parallel_vec = SubprocVecEnv([lambda: make_base_env(ENV_NAME, frame_skip) for _ in range(n_envs)])\n",
        "  stack_vec = VecFrameStack(parallel_vec, n_stack=frame_stack)\n",
        "  return VecMonitor(stack_vec, monitor_log)"
      ],
      "metadata": {
        "id": "4Cynb2wJNeAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2C\n"
      ],
      "metadata": {
        "id": "8M5wz85kPqmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning a Reinforcement Learning algorithm or a ‘hybrid method’ (A2C) that combines value optimization and policy optimization approaches.\n",
        "\n",
        "Policy Based agents directly learn a policy (a probability distribution of actions) mapping input states to output actions. Value Based algorithms learn to select actions based on the predicted value of the input state or action.\n",
        "\n",
        "Both of these methods have considerable drawbacks. That’s why, today, I’ll try another type of Reinforcement Learning method, which we can call a ‘hybrid method’: Actor-Critic. The actor-Critic algorithm is a Reinforcement Learning agent that combines value optimization and policy optimization approaches. More specifically, the Actor-Critic combines the Q-learning and Policy Gradient algorithms. The resulting algorithm obtained at the high level involves a cycle that shares features between:\n",
        "\n",
        "\n",
        "*   Actor: a PG algorithm that decides on an action to take;\n",
        "*   Critic: Q-learning algorithm that critiques the action that the Actor selected, providing feedback on how to adjust. It can take advantage of efficiency tricks in Q-learning, such as memory replay.\n",
        "\n",
        "<img src=https://miro.medium.com/proxy/1*t1rgEDJskqE-iJPg0LI6tw.webp width=\"300\">\n",
        "\n",
        "The advantage of the Actor-Critic algorithm is that it can solve a broader range of problems than DQN, while it has a lower variance in performance relative to REINFORCE. That said, because of the presence of the PG algorithm within it, the Actor-Critic is still somewhat sampling inefficient.\n",
        "\n",
        "The actor critic algorithm consists of two networks (the actor and the critic) working together to solve a particular problem. At a high level, the Advantage Function calculates the agent’s TD Error or Prediction Error. The actor network chooses an action at each time step and the critic network evaluates the quality or the Q-value of a given input state. As the critic network learns which states are better or worse, the actor uses this information to teach the agent to seek out good states and avoid bad states.\n",
        "\n",
        "\n",
        "In my previous tutorial, we derived policy gradients and implemented the REINFORCE algorithm (also known as Monte Carlo policy gradients). There are, however, some issues with vanilla policy gradients: noisy gradients and high variance.\n"
      ],
      "metadata": {
        "id": "GVybpyzk0Ugj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Advantage Actor Critic has two main variants: the Asynchronous Advantage Actor Critic (A3C) and the Advantage Actor Critic (A2C).\n",
        "\n",
        "A3C was introduced in Deepmind’s paper “Asynchronous Methods for Deep Reinforcement Learning” (Mnih et al, 2016). In essence, A3C implements parallel training where multiple workers in parallel environments independently update a global value function—hence “asynchronous.” One key benefit of having asynchronous actors is effective and efficient exploration of the state space.\n",
        "\n",
        "A2C is like A3C but without the asynchronous part; this means a single-worker variant of the A3C. It was empirically found that A2C produces comparable performance to A3C while being more efficient. According to this OpenAI blog post, researchers aren’t completely sure if or how the asynchrony benefits learning:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "After reading the paper, AI researchers wondered whether the asynchrony led to improved performance (e.g. “perhaps the added noise would provide some regularization or exploration?“), or if it was just an implementation detail that allowed for faster training with a CPU-based implementation …\n",
        "\n",
        "Our synchronous A2C implementation performs better than our asynchronous implementations — we have not seen any evidence that the noise introduced by asynchrony provides any performance benefit. This A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Anyhow, we will implement the A2C in this post as it is more simple in implementation. (This can easily be extended to A3C)"
      ],
      "metadata": {
        "id": "oAahgAf8L1HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What’s the advantage function? Considering that “Advantage” is in the Advantage Actor Critic algorithm’s name, it must be pretty important. In order to understand what the Advantage Function is, we first need to understand how to calculate the TD Error, or the Temporal Difference Error.\n",
        "\n",
        "In Temporal Difference Learning, agents learn by making predictions about future rewards and adjusting their actions based on prediction error. One of the reasons Temporal Difference Learning is quite interesting is that prediction error also seems to be one of the ways that the brain learns new things."
      ],
      "metadata": {
        "id": "KMTrvJzjMWa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to calculate the Advantage Function (TD Error), we need to first calculate the TD Target. In the equation above, the TD Target is the predicted value of all future rewards from the current state S. The function V(s’) represents the Critic Network calculating the value of the next state S’.\n",
        "\n",
        "<img src=https://miro.medium.com/max/828/0*SCmsjcou4Sr7dkvs width=\"300\">\n",
        "\n",
        "In the Advantage Actor Critic algorithm, the Advantage is equal to the TD Error shown above. The Advantage can also be interpreted as the Prediction Error of our agent.\n",
        "\n",
        "<img src=https://miro.medium.com/max/640/0*-ZaG1cRzFH2el5f- width=\"200\">\n",
        "\n",
        "Note that the advantage function may not always be the same as the TD Error function. For example, in many Policy Gradient algorithms, the advantage is commonly calculated to be the sum of future discounted rewards shown in Figure 4.\n",
        "\n",
        "The Advantage function tells us if a state is better or worse than expected. If an action is better than expected (the advantage is greater than 0), we want to encourage the actor to take more of that action. If an action is worse than expected (the advantage is less than 0), we want to encourage the actor to take the opposite of that action. If an action performs exactly as expected (the advantage equals 0), the actor doesn’t learn anything from that action.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Q_MDwYOMZRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the algorithm decide which actions to encourage and which to discourage? The A2C algorithm makes this decision by calculating the advantage. The advantage decides how to scale the action that the agent just took. Importantly the advantage can also be negative which discourages the selected action. Likewise, a positive advantage would encourage and reinforce that action."
      ],
      "metadata": {
        "id": "NeqNwt3JM4jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The critic network maps each state to its corresponding Q-value. The Q-value represents the value of a state where Q represents the Quality of the state.\n",
        "Now that we know how to calculate the TD Target and the TD Error, how do we update the Critic Network weights? Note that as the TD Error approaches 0, the Critic Network gets better and better at predicting the outcome from the current state. In this case, we want to drive the TD Error as close to 0 as possible. In order to update the critic network weights, we use the Mean Squared Error of the TD Error function."
      ],
      "metadata": {
        "id": "m8nhb-7iNAnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a2c_n_envs = 16\n",
        "a2c_monitor_log = 'A2C_monitor'\n",
        "a2c_model_name = 'A2C_model'\n",
        "\n",
        "a2c_hyperparams = {\n",
        "  'ent_coef': 0.01,\n",
        "  'vf_coef': 0.25,\n",
        "  'policy_kwargs': dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))\n",
        "}\n",
        "\n",
        "a2c_env = make_vec_env(ENV_NAME, a2c_n_envs, frame_stack, frame_skip, a2c_monitor_log)\n",
        "a2c_model = A2C('CnnPolicy', a2c_env, **a2c_hyperparams, verbose=True)\n",
        "a2c_model.learn(total_timesteps=timesteps, log_interval=1000)\n",
        "a2c_model.save(a2c_model_name)"
      ],
      "metadata": {
        "id": "Vjh0VOxy1P7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO\n"
      ],
      "metadata": {
        "id": "Dnftq2wvQNUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of RL, a policy π is simply a function that returns a feasible action a given a state s. In policy-based methods, the function (e.g., a neural network) is defined by a set of tunable parameters θ. We can adjust these parameters, observe the differences in resulting rewards, and update θ in the direction that yields higher rewards. This mechanism underlies the notion of all policy gradient methods.\n",
        "\n",
        "In summary, policy gradients suffers from major drawbacks:\n",
        "\n",
        "\n",
        "\n",
        "*   Sample inefficiency — Samples are only used once. After that, the policy is updated and the new policy is used to sample another trajectory. As sampling is often expensive, this can be prohibitive. However, after a large policy update, the old samples are simply no longer representative.\n",
        "*   Inconsistent policy updates — Policy updates tend to overshoot and miss the reward peak, or stall prematurely. Especially in neural network architectures, vanishing and exploding gradients are a severe problem. The algorithm may not recover from a poor update.\n",
        "\n",
        "*   High reward variance — Policy gradient is a Monte Carlo learning approach, taking into account the full reward trajectory (i.e., a full episode). Such trajectories often suffer from high variance, hampering convergence. [This issue can be addressed by adding a critic, which is outside of the article’s scope]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AGJ2qMGRYGni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large policy updates.\n",
        "\n",
        "For two reasons:\n",
        "\n",
        "\n",
        "\n",
        "*   We know empirically that smaller policy updates during training are more likely to converge to an optimal solution.\n",
        "*   A too big step in a policy update can result in falling “off the cliff” (getting a bad policy) and having a long time or even no possibility to recover.\n",
        "\n",
        "\n",
        "So with PPO, we update the policy conservatively. To do so, we need to measure how much the current policy changed compared to the former one using a ratio calculation between the current and former policy. And we clip this ratio in a range [1−ϵ,1+ϵ], meaning that we remove the incentive for the current policy to go too far from the old one (hence the proximal policy term).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ue8ubQP5jBKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea was that by taking a gradient ascent step on this function (equivalent to taking gradient descent of the negative of this function), we would push our agent to take actions that lead to higher rewards and avoid harmful actions.\n",
        "\n",
        "However, the problem comes from the step size:\n",
        "\n",
        "\n",
        "*   Too small, the training process was too slow\n",
        "*   Too high, there was too much variability in the training\n",
        "\n",
        "Here with PPO, the idea is to constrain our policy update with a new objective function called the Clipped surrogate objective function that will constrain the policy change in a small range using a clip.\n",
        "\n"
      ],
      "metadata": {
        "id": "nKGNKlxpmm1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We update our policy only if:\n",
        "\n",
        "*   Our ratio is in the range [1−ϵ,1+ϵ]\n",
        "\n",
        "*   Our ratio is outside the range, but the advantage leads to getting closer to the range \n",
        "  *   Being below the ratio but the advantage is > 0\n",
        "  *   Being above the ratio but the advantage is < 0\n",
        "\n",
        "You might wonder why, when the minimum is the clipped ratio, the gradient is 0.\n",
        "\n",
        "To summarize, thanks to this clipped surrogate objective, we restrict the range that the current policy can vary from the old one. Because we remove the incentive for the probability ratio to move outside of the interval since, the clip have the effect to gradient. If the ratio is > \n",
        "1+ϵ or < 1−ϵ the gradient will be equal to 0.\n",
        "\n",
        "The final Clipped Surrogate Objective Loss for PPO Actor-Critic style looks like this, it's a combination of Clipped Surrogate Objective function, Value Loss Function and Entropy bonus:"
      ],
      "metadata": {
        "id": "BOIMYUyBsqcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=https://huggingface.co/blog/assets/93_deep_rl_ppo/ppo-objective.jpg width=\"500\">"
      ],
      "metadata": {
        "id": "NwrJ7QWWnckJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since its introduction in 2017, PPO has quickly established itself as the go-to algorithm in continuous control problems. Five years is a long time in machine learning, yet within its class of benchmark problems (primarily classic control problems and Atari games), it remains highly competitive.\n",
        "\n",
        "It appears PPO strikes the right balance between speed, caution and usability. Although lacking the theoretical guarantees and mathematical finesse that natural gradients and TRPO do, PPO tends to converge faster and better than its competitors. Somewhat paradoxically, it appears that simplicity pays off in Deep Reinforcement Learning."
      ],
      "metadata": {
        "id": "USeX7TVGiSU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_n_envs = 8\n",
        "ppo_monitor_log = 'PPO_monitor'\n",
        "ppo_model_name = 'PPO_model'\n",
        "\n",
        "ppo_hyperparams = {\n",
        "  'n_steps': 128,\n",
        "  'n_epochs': 4,\n",
        "  'batch_size': 256,\n",
        "  'learning_rate': 2.5e-4,\n",
        "  'clip_range': 0.1,\n",
        "  'vf_coef': 0.5,\n",
        "  'ent_coef': 0.01,\n",
        "}\n",
        "\n",
        "ppo_env = make_vec_env(ENV_NAME, ppo_n_envs, frame_stack, frame_skip, ppo_monitor_log)\n",
        "ppo_model = PPO('CnnPolicy', ppo_env, **ppo_hyperparams, verbose=True)\n",
        "ppo_model.learn(total_timesteps=timesteps, log_interval=1000)\n",
        "ppo_model.save(ppo_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3N0MgRVPX1V",
        "outputId": "fa65d7ec-219d-4e07-a2fc-b2560d33ac58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0JPoY_hRPAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CENgNMQVz8YK",
        "TNk5xCxRFHwK",
        "0CbsWHS7pO0t",
        "pxCvl2t3mAnm"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}