{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLTOT13PyXVU"
   },
   "source": [
    "# **Progetto DeepLearning**\n",
    "Lorenzo Gardini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs-wEh_KO9nd"
   },
   "source": [
    "# Implementazione Manuale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlj0roCSJJ3M"
   },
   "source": [
    "## **Project goals**\n",
    "Lo scopo del progetto è quello di studiare, addestrare e comparare algoritmi di Reinforcement Learning applicati al gioco Pong dell'Atari (https://en.wikipedia.org/wiki/Pong). Gli algoritmi utilizzati sono PPO, A2C e DQN con aggiunte per migliorarne le prestazioni (Dueling DQN, Prioritized Experience Replay, Double DQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyqN6abIJQrN"
   },
   "source": [
    "## **Pong**\n",
    "Pong è un videogioco bidimensionale che simula il ping-pong. I due giocatori possono muovere la rispettiva racchetta (simulata da una barra bianca) che può muovere verticalmente lungo un lato dello schermo per colpire la pallina (simulata da un piccolo quadrato) per rispedirla indietro. Se la pallina supera la racchetta di un giocatore il suo avversario guadagna un punto. La palla si muove con velocità costante e può rimbalzare sui lati superiore e inferiore dello schermo.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/max/320/1*P4l2XZUffcJfJQjQ125wSw.gif\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywHWGctblX3r"
   },
   "source": [
    "## **Installazione librerie**\n",
    "- stable-baselines3 è una libreria open source di reinforcement learning. Da questa utiliziamo dei wrapper per l'environment.\n",
    "- ale-py serve per i permessi con i giochi dell'Atari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F_LtjnUptc_J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import statistics\n",
    "import cv2\n",
    "import uuid\n",
    "from base64 import b64encode\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "from abc import abstractmethod, ABC\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ASu-EG241qK"
   },
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgWU0Sf4YKwk"
   },
   "source": [
    "- `print_episode_info`: stampa lo stato corrente dell'apprendimento della rete. \n",
    "- `save_model`: salva il modello (`dqn_action_model`, `dqn_target_model`, `train_rewards`, `epsilon`, `beta`, `replay_memory`) nel `path` specificato\n",
    "- `load_model`: carica il modello (`dqn_action_model`, `dqn_target_model`, `train_rewards`, `epsilon`, `beta`, `replay_memory`) dal `path` specificato per riprendere l'addestramento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ME9_9fOPuGyI"
   },
   "outputs": [],
   "source": [
    "def print_episode_info(episode_number, \n",
    "                       step_count, \n",
    "                       train_step_count, \n",
    "                       episode_start_epsilon, \n",
    "                       epsilon,\n",
    "                       episode_start_beta,\n",
    "                       beta,\n",
    "                       episode_elapsed_time,\n",
    "                       episode_avg_step_time,\n",
    "                       episode_reward,\n",
    "                       moving_avg_reward):\n",
    "  print(f'Episode: {episode_number} Steps: {step_count}[{train_step_count}]',\n",
    "        f'Epsilon: [{episode_start_epsilon:.3f}->{epsilon:.3f}]',\n",
    "        f'Beta: [{episode_start_beta:.3f}->{beta:.3f}]',\n",
    "        f'Time: {episode_elapsed_time:.1f}s[{episode_avg_step_time:.2f}s]',  \n",
    "        f'Total reward: {episode_reward}[{moving_avg_reward:.1f}]')\n",
    "  \n",
    "def save_model(path,\n",
    "               dqn_action_model, \n",
    "               dqn_target_model, \n",
    "               train_rewards, \n",
    "               epsilon, \n",
    "               beta, \n",
    "               replay_memory):\n",
    "  print('saving...')\n",
    "  dynamic_path = lambda x: path + '/' + x\n",
    "  dqn_action_model.save(dynamic_path('dqn_action_model')) \n",
    "  dqn_target_model.save(dynamic_path('dqn_target_model'))\n",
    "  pd.Series(train_rewards).to_csv(dynamic_path('train_rewards.csv'), index=False)\n",
    "  other_data = {\n",
    "      'beta':beta,\n",
    "      'epsilon':epsilon,\n",
    "      'replay_memory':replay_memory\n",
    "  }\n",
    "  with open(dynamic_path('other_data.bin'), 'wb+') as data:\n",
    "    pickle.dump(other_data, data)\n",
    "  print('saved!')\n",
    "\n",
    "def load_model(path):\n",
    "  try:\n",
    "    data = {}\n",
    "    data['dqn_action_model'] = keras.models.load_model(path + '/dqn_action_model')\n",
    "    data['dqn_target_model'] = keras.models.load_model(path + '/dqn_target_model')\n",
    "    data['train_rewards'] = list(pd.read_csv(path + '/train_rewards.csv', squeeze=True, dtype=np.int8))\n",
    "    with open(path + '/other_data.bin', 'rb') as other_data:\n",
    "      data.update(pickle.load(other_data))\n",
    "    print('previous data loaded')\n",
    "    return data\n",
    "  except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print('Data not loaded')\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pNPIbeLvvps"
   },
   "source": [
    "## Specifiche dell'environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-S7hv4FlcOm"
   },
   "source": [
    "Come environment per l'addestramento abbiamo usato la libreria Gym di Open AI (https://www.gymlibrary.dev/environments/atari/pong/#pong). In questa versione di pong la racchetta sinistra è controllata dal computer mentre la racchetta destra può essere mossa manualmente. Sarà proprio questa racchetta che verrà controllata dagli algoritmi. Le azioni   possibili che si possono effettuare sono:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FphtvEvY-UN4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'PongNoFrameskip-v4'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnv69CgZg2RT"
   },
   "source": [
    "Di seguito le azioni possibili in gioco e il loro significati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l9l7cfgP_wzJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'NOOP'),\n",
       " (1, 'FIRE'),\n",
       " (2, 'RIGHT'),\n",
       " (3, 'LEFT'),\n",
       " (4, 'RIGHTFIRE'),\n",
       " (5, 'LEFTFIRE')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(env.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cDmXkxfg5VR"
   },
   "source": [
    "le dimensioni dello spazio dell'environment (le dimensioni del tensore restituito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r6xDfM7S_xG4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0rbjP0ghFC6"
   },
   "source": [
    "I valori minimi e massimi possibili per le osservazioni (essendo un'immagine RGB i valori sono quelli dei pixel per ogni colore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t0snIInP_6ux"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low\n",
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgf_MmfNxDLy"
   },
   "source": [
    "## Environment preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1ChyPzLlVLn"
   },
   "source": [
    "In questa parte viene effettuato il preprocessing dell'environment. Questo viene fatto per migliorare e stabilizzare l'apprendimento della rete. Le varie trasformazioni che vengono applicati all'env sono:\n",
    "- `FrameStack`: prende in ingresso `k` che corrisponde quanti frame impilare per dare alla rete il senso del movimento. L'ultimo stato (quindi lo stato corrente) si trova alla k-esima posizione del tensore.\n",
    "- `StickyActionEnv`: con una probabilità di `action_repeat_probability` l'environment ignora l'azione da eseguire e ripete la precedente azione. Questo permette alla rete (anche con un ϵ molto basso) di mantenere la rete stocastica e più imprevedibile in modo da migliorare l'addestramento (https://arxiv.org/pdf/1709.06009.pdf)\n",
    "- `PreprocessImage`: preprocessa le immagini in modo da ridurre la complessità e ridurre il tempo d'addestramento. L' immagine viene tagliata usando i crops specificati con `crops` che corrisponde ad una tupla (top_crop, right_crop, bottom_crop, left_crop) e poi ridimensionata con le dimensioni specificati `shape` (width, height)\n",
    "- `SkipEnv`: Skippa `skip` frames (per accellerare l'apprendimento).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GF9hkq8tkHVS"
   },
   "outputs": [],
   "source": [
    "class FrameStack(gym.Wrapper):\n",
    "  def __init__(self, env, k):\n",
    "    super().__init__(env)\n",
    "    self.k = k\n",
    "    self.frames = deque([], maxlen=k)\n",
    "    shp = (*env.observation_space.shape, k)\n",
    "    self.observation_space = spaces.Box(low=0, high=255, shape=shp, dtype=env.observation_space.dtype)\n",
    "\n",
    "  def reset(self):\n",
    "    ob = self.env.reset()\n",
    "    for _ in range(self.k):\n",
    "        self.frames.append(ob)\n",
    "    return self._get_ob()\n",
    "\n",
    "  def step(self, action):\n",
    "    ob, reward, done, info = self.env.step(action)\n",
    "    self.frames.append(ob)\n",
    "    return self._get_ob(), reward, done, info\n",
    "\n",
    "  def _get_ob(self):\n",
    "    assert len(self.frames) == self.k\n",
    "    return np.stack(self.frames, axis=2)\n",
    "\n",
    "class StickyActionEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, action_repeat_probability: float) -> None:\n",
    "    super().__init__(env)\n",
    "    self.action_repeat_probability = action_repeat_probability\n",
    "\n",
    "  def reset(self, **kwargs):\n",
    "    self._sticky_action = 0  # NOOP\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "  def step(self, action: int):\n",
    "    if self.np_random.random() >= self.action_repeat_probability:\n",
    "      self._sticky_action = action\n",
    "    return self.env.step(self._sticky_action)\n",
    "\n",
    "class SkipEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, skip: int = 4) -> None:\n",
    "    super().__init__(env)\n",
    "    self._skip = skip\n",
    "\n",
    "  def step(self, action: int):\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    for _ in range(self._skip):\n",
    "      obs, reward, done, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "          break\n",
    "    return obs, total_reward, done, info\n",
    "\n",
    "class PreprocessImageEnv(gym.ObservationWrapper):\n",
    "  def __init__(self, env: gym.Env, shape=(84,84), crops=(None, None, None, None)):\n",
    "    super().__init__(env)\n",
    "    self.shape = shape\n",
    "    self.crops = crops\n",
    "    self.observation_space = spaces.Box(\n",
    "        low=0, high=255, shape=shape, dtype=env.observation_space.dtype\n",
    "    )\n",
    "    obs_dims = self.env.observation_space\n",
    "    self.height = obs_dims.shape[0]\n",
    "    self.width = obs_dims.shape[1]\n",
    "    self.obs = np.empty((self.height, self.width), dtype=np.uint8)\n",
    "\n",
    "  def observation(self, frame: np.ndarray) -> np.ndarray:\n",
    "    self.env.ale.getScreenGrayscale(self.obs)\n",
    "    height, width = self.obs.shape\n",
    "\n",
    "    top_crop, right_crop, bottom_crop, left_crop = self.crops\n",
    "    top_crop = 0 if top_crop is None else top_crop\n",
    "    right_crop = width if right_crop is None else right_crop\n",
    "    bottom_crop = height if bottom_crop is None else bottom_crop\n",
    "    left_crop = 0 if left_crop is None else left_crop\n",
    "\n",
    "    frame = self.obs[left_crop:right_crop, top_crop:bottom_crop]\n",
    "    frame = cv2.resize(frame, self.shape, interpolation=cv2.INTER_AREA)\n",
    "    int_image = np.asarray(frame, dtype=np.uint8)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuxeUdqw5GXt"
   },
   "source": [
    "Questa funzione permette di creare l'environment specificato applicando i wrappers elencati prima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hun2V_P9aMno"
   },
   "outputs": [],
   "source": [
    "def make_env(env_name, stacked_frames, frame_skip, action_repeat_probability, shape, crops):\n",
    "  env = gym.make(env_name)\n",
    "  env = StickyActionEnv(env, action_repeat_probability)\n",
    "  env = SkipEnv(env, skip=frame_skip)\n",
    "  env = PreprocessImageEnv(env, shape=shape, crops=crops)\n",
    "  env = FrameStack(env, k=stacked_frames)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbKSgS8LiiFv"
   },
   "source": [
    "## Struttura della rete (Dueling DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6UM6pzV5p8T"
   },
   "source": [
    "Rete introdotta nel 2015 da DeepMind (https://arxiv.org/pdf/1511.06581.pdf).\n",
    "Per capire come funziona bisogna prima considerare che il Q-Value di una coppia $(s, a)$ può essere espressa come $Q(s, a) = V(s) + A(s, a)$ dove $V(s)$ corrisponde al valore dell'azione $s$ mentre $A(s, a)$ corrisponde al _**vantaggio**_ di effettuare l'azione $s$ nello stato $a$ rispetto a tutte le altre possibili azioni in quello stato. Inoltre, il valore di uno stato è uguale al Q-Value della migliore azione $a^*$ per quello\n",
    "stato (poiché assumiamo che la politica ottimale scelga l'azione migliore), quindi $V(s) = Q(s, a^*)$, il che implica che $A(s, a^*) = 0$. Nel Dueling DQN il modello stima sia\n",
    "il valore dello stato e il vantaggio di ogni possibile azione. Dal momento che l'azione migliore dovrebbe avere un vantaggio pari a 0, il modello sottrae il massimo vantaggio previsto da tutti i vantaggi previsti. Questo deriva dal fatto che la somma di $V$ e $A$ è \"non identificabile\" in quanto dato $Q$ non è possibile recuperare $V$ e $A$ in modo univoco. Sottrarre il massimo di $A$ forza $Q$ ad essere uguale a $V$. \n",
    "\n",
    "Ecco come si configura la rete:\n",
    "<p align=\"center\">\n",
    "  <img src='https://www.fromkk.com/images/ddqn_duel_dqn.png' margin='center'/>\n",
    "</p>\n",
    "\n",
    "Sopra la classica configurazione CNN della rete DQN.\n",
    "Sotto la configurazione di una rete Dueling. L'ultimo layer convoluzonale viene appiattito con il layer `Flatten` e viene collegato con due layer `Dense` di 512 neuroni ciascuno. $A$ viene modellato con un layer di $n$ neuroni pari al numero di azioni possibili del gioco mentre $V$ è un singono neurone denso. Infine i due valori vengono combinati per calcolare i Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bJQgK-SNTjMk"
   },
   "outputs": [],
   "source": [
    "def build_dqn(input_shape=(84, 84, 4),action_count=9):\n",
    "    model=keras.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=input_shape,name='Input'),\n",
    "                layers.Conv2D(filters=32, kernel_size=8, strides=4,activation='relu',padding='valid',name='C1'),\n",
    "                layers.Conv2D(filters=64, kernel_size=4,strides=2,activation='relu',padding='valid',name='C2'),\n",
    "                layers.Conv2D(filters=64, kernel_size=3,strides=1,activation='relu',padding='valid',name='C3'),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(512, activation='relu',name='FC'),\n",
    "                layers.Dense(units=action_count,activation='linear',name='Output')\n",
    "            ]\n",
    "        )\n",
    "    return model\n",
    "\n",
    "def build_dueling_dqn(input_shape=(84, 84, 4), action_count=9): \n",
    "  input = layers.Input(shape=input_shape, name='input')\n",
    "  conv_1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, padding='valid', activation='relu')(input)\n",
    "  conv_2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, padding='valid', activation='relu')(conv_1)\n",
    "  conv_3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu')(conv_2)\n",
    "  flatten = layers.Flatten()(conv_3)\n",
    "  # branch state\n",
    "  flatten_state = layers.Dense(512, activation='relu')(flatten)\n",
    "  state = layers.Dense(1, activation='linear')(flatten_state)\n",
    "  # branch advantages\n",
    "  flatten_advantages = layers.Dense(512, activation='relu')(flatten)\n",
    "  raw_advantages = layers.Dense(action_count,activation='linear')(flatten_advantages) \n",
    "\n",
    "  # action score - mean action score = centered action score\n",
    "  advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1, keepdims=True)\n",
    "  Q_values = state + advantages\n",
    "\n",
    "  model = keras.Model(inputs=[input], outputs=[Q_values])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTiCG9w_pu1J"
   },
   "source": [
    "## Prioritized Experience Replay (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BruEMd7QDMgi"
   },
   "source": [
    "Soluzione presentata da DeepMind nel 2015 per migliorare le prestazioni di DQN (https://arxiv.org/pdf/1511.05952.pdf). L'idea è quella che invece di campionare le esperienze in modo uniforme dal buffer di riproduzione vengono campionate le esperienze importanti più frequentemente. Le esperienze sono considerate \"importanti\" se possono condurre a rapidi progressi nell'apprendimento. Poiché i campioni saranno sbilanciati verso le esperienze importanti si compensa questa distorsione durante l'addestramento riducendo il peso delle esperienze in base alla loro importanza, altrimenti il modello si overfitterà le esperienze importanti. Per fare ciò si da loro un peso inferiore durante l'allenamento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EhO830bZm5I"
   },
   "source": [
    "### **Calcolo delle probabilità**\n",
    "La probabilità di campionamento di una trasizione i è:\n",
    "$$ P(i) = \\frac{p_i^\\alpha}{\\sum_k{p_k^\\alpha}}$$\n",
    "\n",
    "dove $\\alpha$ è un iperparametro che determina la quantità di priorità utilizzata, con $\\alpha=0$ corrispondente al caso uniforme. $p_i$ è la priorità.\n",
    "\n",
    "Per fare in modo che la rete non overfitti si corregge il bias utilizzando dei pesi $w$ nella loss function:\n",
    "\n",
    "$$ w_i = (N \\cdot P(i))^{-\\beta}$$\n",
    "\n",
    "dove $N$ è il numero di esperienze nel buffer e $\\beta$ è un iperparametro che controlla quanto compensare il bias (0 significa per niente, mentre 1 significa del tutto). Nel paper si usa $\\beta = 0.4$ all'inizio dell'addestramento che aumenta linearmente fino a $\\beta = 1$\n",
    "\n",
    "I $w_i$ vengono normalizzati con $\\frac{1}{max_i \\; w_i}$ per stabilità.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJLKDnh5XxNG"
   },
   "source": [
    "### **Binary Segment Tree**\n",
    "Innanzitutto, si può semplicemente implementare PER ordinando in base alla priorità. Ciò non sarà efficiente a causa di $O(n \\cdot log(n))$ per l'inserimento e $O(n)$ per il campionamento.\n",
    "Per effettuare un campionamento pesato in modo efficiente si utilizza una struttura dati chiamata **Binary Segment Tree**.\n",
    "\n",
    "Viene usato per calcolare velocemente $\\sum_k^i{p_k^a}$ e $min \\; p_i^\\alpha$ che serve per calcolare $\\frac{1}{max_i \\; w_i}$. Per effettuare questi calcoli occorrono un **SumTree** e un **MinTree**. Entrambi sono dei **Binary Segment Tree**.\n",
    "\n",
    "Il Binary Segment Tree consente di effettuare gli inserimenti con $O(log(n)$ e i calcoli in $O(log(n))$, che sono molto più efficienti di $O(n \\cdot log(n))$ e $O(n)$\n",
    "\n",
    "Sia $x_i$ la lista di $N$ valori che si vogliono rappresentare. Sia $b_{i,j}$ il $j^{th}$ nodo della $i^{th}$ riga dell'albero binario. I figlio del nodo $b_{i,j}$ sono $b_{i+1,2j}$ $b_{i+1,2j+1}$. \n",
    "\n",
    "Le foglie in posizione $D = \\lceil 1+ log_2N  \\rceil$ hanno i valori di $x$. Ogni nodo mantiene la somma dei valori dei nodi figli. La radice quindi contiene la somma dell'intero array di valori. Il figlio sinistro e destro della radice mantengono la somma rispettivamente della prima metà e della seconda dell'array.\n",
    "\n",
    "$$ b_{i,j} = \\displaystyle\\sum_{k=(j-1) * 2^{D-i} + 1} ^ {j*2^{D-i}} x^k$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='https://pylessons.com/media/Tutorials/Reinforcement-learning-tutorial/CartPole-PER/SumTree.png' width='800' />\n",
    "</p>\n",
    "\n",
    "Il numero di nodi in ogni riga $i$ sono:\n",
    "\n",
    "$$N_i = \\left\\lceil\\dfrac{N}{D-i+1}\\right\\rceil $$\n",
    "\n",
    "Questo è equivalente alla somma di tutte le righe sotto $i$. Quindi basta usare un array per salvare tutto l'albero dove \n",
    "\n",
    "$$b_{i,j} \\rightarrow a_{N_i + j}$$\n",
    "\n",
    "I figli di $a_i$ sono $a_{2i}$ e $a_{2i +1}$ tali che:\n",
    "\n",
    "$$a_i = a_{2i} + a_{2i +1}$$\n",
    "\n",
    "N.B. Gli indici iniziano da 1 e non da 0 (per gestire facilmente l'albero)\n",
    "\n",
    "N.B. Gli stessi ragionamenti vengono effettuati per creare un **Min Heap**. In questo caso al posto dlla somma si usa l'operatore minimo.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNVfKsvqUShk"
   },
   "source": [
    "### **Come si fa sampling?**\n",
    "1. Si genera un numero random tra 0 e la radice in modo uniforme.\n",
    "2. Si confronta il valore con il nodo di sinistra, Se il valore è minore si scende lungo il nodo e si ripete il punto 2. Se il valore è maggiore allora si scende al nodo di destra sottraendo il valore del nodo di sinistra e si ripete il punto 2.\n",
    "3. Si raggiunge una foglia. Questo valore corrisponde ad una priorità di una transizione.\n",
    "4. Si ottiene l'indice della transizione sottraendo all'indice dell'albero la capacità del vettore.\n",
    "6. Si ripetono i passi da 1. a 6. finchè non si ottiene il mini-batch della grandezza desiderata.\n",
    "5. Vengono restituiti al chiamante il mini-batch delle transizioni, i corrispettivi indici (verranno utilizzati per aggiornare le priorità delle transizioni dopo la backpropagation) e i corrispettivi $w_i$ (per eliminare il bias durante la backpropagation)\n",
    "<p align=\"center\">\n",
    "  <img src='http://www.sefidian.com/wp-content/uploads/2022/05/image-15.png' width='500' />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "682Gd_4xf44c"
   },
   "source": [
    "### **Inserimento nuova transizione**\n",
    "\n",
    "La nuova transizione viene aggiunta con il valore di priorità impostato come iperparametro, solitamente = 1 e poi vengono aggiornati i due alberi propagando il valore dalle foglie al nodo radice e applicando la funzione specifica dell'albero (somma e minimo).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KEYdehshGtF"
   },
   "source": [
    "### **Utilizzo dei $w_i$ e aggiornamento delle priorità**\n",
    "\n",
    "Dato che il sampling pesato crea un bias allora l'aggiornamento della rete viene pesato utilizzando i $w_i$. Questi valori vengono moltiplicati al gradiente prima di effettuare la backpropagation.\n",
    "\n",
    "$$\\Delta \\leftarrow \\Delta + w_i \\cdot \\delta_j \\cdot \\nabla_\\theta Q(S_{j-1}, A_{j-1})$$\n",
    "\n",
    "con $ j $ indice che scorre i $k$ elementi di un minibatch\n",
    "\n",
    "I $\\delta_j$ vengono utilizzati per aggiornare le priorità: se una transizione ha generato poco errore allora la sua priorità deve essere diminuita, viceversa se ha generato un errore grande. Le priorità si aggiornano utilizzando gli indici ritornati durante la fase di sampling. L'aggiornamento si effettua cosi:\n",
    " \n",
    " $$p_i = |\\delta_i| + \\epsilon $$\n",
    " Dove $\\delta_i$ è l'errore commesso dalla i-esima transizione e $\\epsilon$ è un iperarametro (solitamente messo a 0.1 o 0.01) che crea una base per fare in modo che la probabilità di campionare una transizione non diventi mai 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dKFqyrbJBXs"
   },
   "source": [
    "### **Implementazione**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCIB4c6UJgjR"
   },
   "source": [
    "Implementazione della classe astratta di una ReplayMemory. \n",
    "- `append`: aggiunge una transizione alla reply memory\n",
    "- `sample_minibatch`: seleziona un mini-batch delle dimensioni `batch_size` e lo divide con `_split_minibatch`\n",
    "- metodo statico privato `_split_minibatch`: splitta un batch di transitions in 5 batch `[state_batch, action_batch, reward_batch, new_state_batch, done_batch]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_I0LzizcJf5U"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(ABC):\n",
    "  @abstractmethod\n",
    "  def __len__(self):\n",
    "    pass\n",
    "  @abstractmethod\n",
    "  def append(self, transition):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def sample_minibatch(self, batch_size):\n",
    "    pass\n",
    "\n",
    "  @staticmethod\n",
    "  def _split_minibatch(minibatch):\n",
    "    state_batch = np.array([sample[0] for sample in minibatch])\n",
    "    action_batch = np.array([sample[1] for sample in minibatch])\n",
    "    reward_batch = np.array([sample[2] for sample in minibatch])\n",
    "    new_state_batch = np.array([sample[3] for sample in minibatch])\n",
    "    done_batch = np.array([sample[4] for sample in minibatch])\n",
    "\n",
    "    return [state_batch,action_batch,reward_batch,new_state_batch,done_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3oS-lASK33D"
   },
   "source": [
    "Implementazione della Replay Memory base. Implementata come una lista circolare perchè la classe deque è molto lenta in lettura per dimensioni molto grandi (è implementata come una doppia liked list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wCch-g3qK2wo"
   },
   "outputs": [],
   "source": [
    "class CircularReplayMemory(ReplayMemory):\n",
    "  def __init__(self, max_size):\n",
    "    self._buffer = np.empty(max_size, dtype=object)\n",
    "    self.max_size = max_size\n",
    "    self.index = 0\n",
    "    self.size = 0\n",
    "\n",
    "  def append(self, transition):\n",
    "    self._buffer[self.index] = transition\n",
    "    self.size = min(self.size + 1, self.max_size)\n",
    "    self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    return self._buffer[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def sample_minibatch(self, batch_size):\n",
    "    minibatch_indices = np.random.choice(range(self.size), size=batch_size)\n",
    "    minibatch = [self._buffer[i] for i in minibatch_indices]\n",
    "    return self._split_minibatch(minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LZmACmUQ7gl"
   },
   "source": [
    "Classe astratta per una generica Prioritized Replay Memory.\n",
    "\n",
    "- `sample_minibatch`: ora il sampling del mini-batch riceve come parametro aggiuntivo $\\beta$ per calcolare i $w_i$. Oltre ai minibatch di *state, reward, done, new_state* ritorna anche gli indici $i$ e i pesi $w_i$ corrispondenti alle transizioni selezionate\n",
    "- `update_priorities`: funzione che aggiorna le priorità delle transizioni nelle posizioni `indexes`. `losses` sono i $\\delta_i$ computati durante l'aggiornamento della rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5kBuKioAQ67U"
   },
   "outputs": [],
   "source": [
    "class PrioritizedMemory(ReplayMemory):\n",
    "  @abstractmethod\n",
    "  def sample_minibatch(self, batch_size, beta):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def update_priorities(self, indexes, losses):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzjStDMscOPW"
   },
   "source": [
    "### Implementazione PER\n",
    "Implementazione della PER memory utilizzando gli alberi per ottimizzare i calcoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FuebudfBpeFG"
   },
   "outputs": [],
   "source": [
    "class TreePrioritizedReplayMemory(PrioritizedMemory):\n",
    "  def __init__(self, capacity, alpha=0.6, error_offset=0.1):\n",
    "    self._capacity = self._compute_power_2_capacity(capacity)\n",
    "    self._alpha = alpha\n",
    "    self._error_offset = error_offset\n",
    "    # Maintain segment binary trees to take sum and find minimum\n",
    "    self._priority_sum = np.zeros(2 * self._capacity)\n",
    "    self._priority_min = np.full(2 * self._capacity, np.inf)\n",
    "    self._data = np.full(self._capacity, None)\n",
    "    # Current max priority, p, to be assigned to new transitions\n",
    "    self._max_priority = 1.\n",
    "    # We use cyclic buffers to store data, and next_idx keeps the index of the next empty slot\n",
    "    self._next_idx = 0\n",
    "    # Size of the buffer\n",
    "    self._size = 0\n",
    "\n",
    "  def __len__(self):\n",
    "    return self._size\n",
    "\n",
    "  def append(self, transition):\n",
    "    # Get next available slot\n",
    "    idx = self._next_idx\n",
    "    # store in the queue\n",
    "    self._data[idx] = transition\n",
    "    # Increment next available slot\n",
    "    self._next_idx = (idx + 1) % self._capacity\n",
    "    # Calculate the size\n",
    "    self._size = min(self._capacity, self._size + 1)\n",
    "    # p_i ^ alpha, new samples get max_priority\n",
    "    priority_alpha = self._max_priority ** self._alpha\n",
    "    # Update the two segment trees for sum and minimum\n",
    "    self._set_priority_min(idx, priority_alpha)\n",
    "    self._set_priority_sum(idx, priority_alpha)\n",
    "\n",
    "  def sample_minibatch(self, batch_size, beta):\n",
    "    # to normalize weights we divide by 1/max_w for stability\n",
    "    # the next two lines computes max_w\n",
    "    # p_i ^ alpha / sum(p ^ alpha) => min prob = 1/min prob => the biggest fraction\n",
    "    prob_min = self._min() / self._sum() # P(i)\n",
    "    # (min P(i) * N) ^ (-beta) = (1/N * 1/P(i)) ^ beta => compute max w \n",
    "    max_weight = (prob_min * self._size) ** (-beta)\n",
    "\n",
    "    # Initialize samples\n",
    "    weights = np.zeros(batch_size, dtype=np.float32)\n",
    "    # Get sample indices\n",
    "    indices = np.array([self._find_prefix_sum_idx(p) for p in np.random.random(size=batch_size) * self._sum()])   \n",
    "\n",
    "    for i in range(batch_size):\n",
    "      idx = indices[i]\n",
    "      # P(i) = p_i ^ alpha / (sum p^alpha)\n",
    "      prob = self._priority_sum[idx + self._capacity] / self._sum()\n",
    "      # (P(i) * N)^(-beta) = (1/N * 1/P(i)) ^ beta\n",
    "      weight = (prob * self._size) ** (-beta)\n",
    "      # Normalize by 1/max_weight which also cancels off the 1/N term\n",
    "      weights[i] = weight / max_weight\n",
    "\n",
    "    return (*self._split_minibatch(self._data[indices]), weights, indices)\n",
    "\n",
    "  def update_priorities(self, indexes, losses):\n",
    "    priorities = np.abs(losses) + self._error_offset\n",
    "    # Set current max priority\n",
    "    self._max_priority = max(self._max_priority, max(priorities))\n",
    "\n",
    "    for idx, priority in zip(indexes, priorities):\n",
    "      # Calculate p_i^alpha\n",
    "      priority_alpha = priority ** self._alpha\n",
    "      # Update the trees\n",
    "      self._set_priority_min(idx, priority_alpha)\n",
    "      self._set_priority_sum(idx, priority_alpha)\n",
    "\n",
    "  def _compute_power_2_capacity(self, capacity):\n",
    "    # convert capacity to a power of 2\n",
    "    new_capacity = 1\n",
    "    while new_capacity < capacity:\n",
    "        new_capacity *= 2\n",
    "    return new_capacity\n",
    "\n",
    "  def _set_priority_min(self, idx, priority_alpha):\n",
    "    # Leaf of the binary tree\n",
    "    idx += self._capacity\n",
    "    self._priority_min[idx] = priority_alpha\n",
    "    # Update tree, by traversing along ancestors. Continue until the root of the tree.\n",
    "    while idx >= 2:\n",
    "      # Get the index of the parent node\n",
    "      idx //= 2\n",
    "      # Value of the parent node is the minimum of it's two children\n",
    "      self._priority_min[idx] = min(self._priority_min[2 * idx], self._priority_min[2 * idx + 1])\n",
    "\n",
    "  def _set_priority_sum(self, idx, priority):\n",
    "    # Leaf of the binary tree\n",
    "    idx += self._capacity\n",
    "    # Set the priority at the leaf\n",
    "    self._priority_sum[idx] = priority\n",
    "    # Update tree, by traversing along ancestors. Continue until the root of the tree.\n",
    "    while idx >= 2:\n",
    "      # Get the index of the parent node\n",
    "      idx //= 2\n",
    "      # Value of the parent node is the sum of it's two children\n",
    "      self._priority_sum[idx] = self._priority_sum[2 * idx] + self._priority_sum[2 * idx + 1]\n",
    "\n",
    "  def _sum(self):\n",
    "    # The root node keeps the sum of all values\n",
    "    return self._priority_sum[1]\n",
    "\n",
    "  def _min(self):\n",
    "    # The root node keeps the minimum of all values\n",
    "    return self._priority_min[1]\n",
    "\n",
    "  def _find_prefix_sum_idx(self, prefix_sum):\n",
    "    # find largest i such as p_1^alpha + p_2^alpha + ... p_i^alpha <= prefix_sum = P\n",
    "    # Start from the root\n",
    "    idx = 1\n",
    "    while idx < self._capacity:\n",
    "      # If the sum of the left branch is higher than required sum\n",
    "      if self._priority_sum[idx * 2] > prefix_sum:\n",
    "        # Go to left branch of the tree\n",
    "        idx = 2 * idx\n",
    "      else:\n",
    "        # Otherwise go to right branch and reduce the sum of left branch from required sum\n",
    "        prefix_sum -= self._priority_sum[idx * 2]\n",
    "        idx = 2 * idx + 1\n",
    "    # We are at the leaf node. Subtract the capacity by the index in the tree to get the index of actual value in the array\n",
    "    return idx - self._capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj1z1DHSkYy_"
   },
   "source": [
    "Funzione che permette di riempire con `replay_memory_init_size` observations ricavate effettuando azioni random. Ogni episodio termina naturalmente o quando viene raggiunto un numero `episode_max_steps` di passi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jTADuafxN0nA"
   },
   "outputs": [],
   "source": [
    "def dqn_replay_memory_init(env, replay_memory, replay_memory_init_size, episode_max_steps):\n",
    "  while True:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    print_step = math.ceil(replay_memory_init_size / 10)\n",
    "    while step_count < episode_max_steps and not done:\n",
    "      action = env.action_space.sample()\n",
    "      new_state, reward, done, _ = env.step(action)\n",
    "      replay_memory.append([state,action,reward,new_state,done])\n",
    "      state = new_state\n",
    "      step_count += 1\n",
    "      replay_memory_len = len(replay_memory)\n",
    "      if replay_memory_len % print_step:\n",
    "        print(f'\\r{replay_memory_len}/{replay_memory_init_size}', end='')\n",
    "      if replay_memory_len >= replay_memory_init_size:\n",
    "        print()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RMnsTDC0E2J"
   },
   "source": [
    "## Action model update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5GRIzdXk85m"
   },
   "source": [
    "In questa parte viene mostrato come la rete si aggiorna. \n",
    "L'aggiornamento della rete viene effettuato utilizzando un'implementazione chiamata Double DQN (https://arxiv.org/pdf/1509.06461.pdf) pubblicata da DeepMind nel 2015 per stabilizzare e migliorare l'addestramento. \n",
    "\n",
    "Il miglioramento si basa sul fatto che la rete _target_ tende a sovrastimare i Q-Value. Si supponga che tutte le azioni siano ugualmente buone: i Q-Values stimati dal modello target dovrebbero essere identici, ma trattandosi di approssimazioni, alcune potrebbero essere leggermente maggiori di altre, per puro caso. Il modello target selezionerà sempre il Q-Value più grande, che sarà leggermente maggiore del Q-Value medio, molto probabilmente sovrastimando il vero Q-Value. Per risolvere questo problema, hanno proposto di utilizzare il modello online (_action_model_) anziché il target quando si selezionano le azioni migliori per gli stati successivi e di utilizzare solo il modello di target per stimare i Q-Value per queste migliori azioni.\n",
    "\n",
    "Il valore target diventa da:\n",
    "$$Y^{DQN}_t \\equiv R_{t+1} + \\gamma \\cdot \\max_{a} Q(S_{t+1}, a; \\theta_t^-) $$\n",
    "a:\n",
    "$$Y^{DoubleQ}_t \\equiv R_{t+1} + \\gamma \\cdot Q(S_{t+1}, \\underset{x}{\\mathrm{argmax}}Q(S_{t+1}, a; \\theta_t); \\theta_t^-) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPKAnM9anCdq"
   },
   "source": [
    "`update_action_model` è una funzione generica che astrae dalla tipologia di approccio utilizzato perche in ingresso prende i soliti parametri (`state_batch`, `action_batch`, `reward_batch`, `done_batch`, `gamma` e `dqn_action_model`), i `weights` (utilizzati nel caso si usi PER) e `target_new_state_q_values`, calcolati indipendentemente dall'algoritmo scelto.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0T15XH4TrOgV"
   },
   "outputs": [],
   "source": [
    "def update_action_model(dqn_action_model, \n",
    "                        target_new_state_q_values, \n",
    "                        gamma,\n",
    "                        state_batch, \n",
    "                        action_batch, \n",
    "                        reward_batch, \n",
    "                        done_batch,\n",
    "                        weights):\n",
    "\n",
    "  # find the action model Q values for all possible actions given the current state batch\n",
    "  predicted_state_q_values = dqn_action_model.predict(state_batch, verbose=False)\n",
    "  predicted_Q_values_copy = predicted_state_q_values.copy()\n",
    "  # estimate the target values y_i\n",
    "  # for the action we took, use the target model Q values  \n",
    "  # for other actions, use the action model Q values\n",
    "  # in this way, loss function will be 0 for other actions\n",
    "  for i,(a,r,new_state_q_values, done) in enumerate(zip(action_batch,reward_batch,target_new_state_q_values, done_batch)): \n",
    "      if not done:  \n",
    "        target_value = r + gamma * np.amax(new_state_q_values)\n",
    "      else:         \n",
    "        target_value = r\n",
    "      predicted_state_q_values[i][a] = target_value #y_i\n",
    "  \n",
    "  losses = dqn_action_model.loss(predicted_state_q_values, predicted_Q_values_copy)  \n",
    "  # update weights of action model using the train_on_batch method \n",
    "  dqn_action_model.train_on_batch(state_batch, predicted_state_q_values, sample_weight=weights)\n",
    "  \n",
    "  return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLFmkBFktkCq"
   },
   "source": [
    "Computa i *target_new_state_q_values* con DoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qG8D9vnK0l5L"
   },
   "outputs": [],
   "source": [
    "def double_dqn_update(dqn_action_model, dqn_target_model, mini_batch, gamma, weights=None):\n",
    "  # the transition mini-batch is divided into a mini-batch for each element of a transition\n",
    "  state_batch,action_batch,reward_batch,new_state_batch,done_batch=mini_batch\n",
    "\n",
    "  # pixel values are normalized in the range [0;1]\n",
    "  norm_state_batch = state_batch/255.0\n",
    "  norm_new_state_batch = new_state_batch/255.0\n",
    "\n",
    "  # find the target model Q values for all possible actions given the new state batch\n",
    "  next_Q_values = dqn_action_model.predict(norm_new_state_batch, verbose=0)  # not target.predict()\n",
    "  best_next_actions = next_Q_values.argmax(axis=1) # select best action based on action model\n",
    "  next_mask = tf.one_hot(best_next_actions, next_Q_values.shape[1]) # mask for best actions\n",
    "  target_new_state_q_values = dqn_target_model.predict(norm_new_state_batch, verbose=0) * next_mask\n",
    "  \n",
    "  return update_action_model(dqn_action_model, \n",
    "                             target_new_state_q_values, \n",
    "                             gamma, \n",
    "                             norm_state_batch, \n",
    "                             action_batch, \n",
    "                             reward_batch, \n",
    "                             done_batch, \n",
    "                             weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0ZadIStb6j1"
   },
   "source": [
    "### Altre funzioni di aggiornamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNByYwa0cESU"
   },
   "source": [
    "Funzione che calcola i target Q_values in FixedDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8IcYVuqAbn6_"
   },
   "outputs": [],
   "source": [
    "def dqn_action_target_update(dqn_action_model, dqn_target_model, mini_batch, gamma, weights=None):\n",
    "  # the transition mini-batch is divided into a mini-batch for each element of a transition\n",
    "  state_batch,action_batch,reward_batch,new_state_batch,done_batch=mini_batch\n",
    "\n",
    "  # pixel values are normalized in the range [0;1]\n",
    "  norm_state_batch = state_batch/255.0\n",
    "  norm_new_state_batch = new_state_batch/255.0\n",
    "\n",
    "  # find the target model Q values for all possible actions given the new state batch\n",
    "  target_new_state_q_values = dqn_target_model.predict(norm_new_state_batch, verbose=False)\n",
    "  \n",
    "  return update_action_model(dqn_action_model, \n",
    "                             target_new_state_q_values, \n",
    "                             gamma, \n",
    "                             norm_state_batch, \n",
    "                             action_batch, \n",
    "                             reward_batch, \n",
    "                             done_batch, \n",
    "                             weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLNtlUYRcK0m"
   },
   "source": [
    "Funzione che calcola i target Q-Values nella classica versione DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iZ7_Jldhb2c4"
   },
   "outputs": [],
   "source": [
    "def dqn_update(dqn_action_model, loss_fn, mini_batch, gamma, weights=None):\n",
    "  # the transition mini-batch is divided into a mini-batch for each element of a transition\n",
    "  state_batch,action_batch,reward_batch,new_state_batch,done_batch=mini_batch\n",
    "\n",
    "  # pixel values are normalized in the range [0;1]\n",
    "  norm_state_batch = state_batch/255.0\n",
    "  norm_new_state_batch = new_state_batch/255.0\n",
    "\n",
    "  # find the target model Q values for all possible actions given the new state batch\n",
    "  target_new_state_q_values = dqn_action_model.predict(norm_new_state_batch, verbose=False)\n",
    "  \n",
    "  return update_action_model(dqn_action_model, \n",
    "                             target_new_state_q_values,\n",
    "                             gamma, \n",
    "                             norm_state_batch, \n",
    "                             action_batch, \n",
    "                             reward_batch, \n",
    "                             done_batch, \n",
    "                             weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBZN5wv0qeST"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyvtBzEwc3T1"
   },
   "source": [
    "Funzione di addestramento della rete. Addestra una rete DoubleDQN, ma basta effettuare qualche modifica per ottenere le altre.\n",
    "- episode_count: numero di episodi con cui addestrare la rete\n",
    "- env: l'environment\n",
    "- dqn_action_model: l'online model\n",
    "- dqn_target_model: l'offline model (potrebbe essere lo stesso `dqn_action_model`)\n",
    "- episode_max_steps: numero di steps massimi per episodio,\n",
    "- replay_memory_max_size: grandezza massima della reply memory,\n",
    "- replay_memory_init_size: numero di transition con cui inizializzare la replay memory,\n",
    "- alpha: quantità di priorità utilizzare,\n",
    "- error_offset: offset sommato all'errore quando si aggiorna la PRM,\n",
    "- batch_size: grandezza del mini-batch da campionare,\n",
    "- step_per_update: ogni quanti step aggiornare l'online model,\n",
    "- step_per_update_target_model: ogni quanti step aggiornare l'offline model,\n",
    "- max_epsilon: $\\epsilon$ massima,\n",
    "- min_epsilon: $\\epsilon$ minima,\n",
    "- epsilon_decay: quanto far decadere $\\epsilon$ in ogni step,\n",
    "- gamma: il fattore di sconto $\\gamma$,\n",
    "- max_beta: $\\beta$ massimo,\n",
    "- min_beta: $\\beta$ minimo,\n",
    "- beta_increase: quanto incrementare $\\beta$ ogni step,\n",
    "- moving_avg_window_size: la finestra di reward con cui calcolare la media mobile,\n",
    "- moving_avg_stop_thr: se `moving_avg_window_size` raggiunge questo valore il compito viene considerato completato e l'addestramento termina,\n",
    "- load_folder: path dove vengono caricati i dati,\n",
    "- save_folder: path dove salvare i dati,\n",
    "- save_freq: ogni quanti episodi salvare il modello e i dati annessi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TRAUHunNiQ0p"
   },
   "outputs": [],
   "source": [
    "def dqn_training(episode_count,\n",
    "                 env,\n",
    "                 dqn_action_model,\n",
    "                 dqn_target_model,\n",
    "                 episode_max_steps=np.inf,\n",
    "                 replay_memory_max_size=100_000,\n",
    "                 replay_memory_init_size=20_000,\n",
    "                 alpha=0.6,\n",
    "                 error_offset=0.1,\n",
    "                 batch_size=32,\n",
    "                 step_per_update=4,\n",
    "                 step_per_update_target_model=10_000,\n",
    "                 max_epsilon=1,\n",
    "                 min_epsilon=0.1,\n",
    "                 epsilon_decay=1e-6,\n",
    "                 gamma=0.99,\n",
    "                 max_beta=1,\n",
    "                 min_beta=0.4,\n",
    "                 beta_increase=1e-6,\n",
    "                 moving_avg_window_size=100,\n",
    "                 moving_avg_stop_thr=None,\n",
    "                 load_folder=None,\n",
    "                 save_folder=None,\n",
    "                 save_freq=50):\n",
    "  \n",
    "  data = load_model(load_folder) if load_folder else {}\n",
    "  dqn_action_model = data.get('dqn_action_model', dqn_action_model)\n",
    "  dqn_target_model = data.get('dqn_target_model', dqn_target_model)\n",
    "  replay_memory = data.get('replay_memory', TreePrioritizedReplayMemory(replay_memory_max_size, alpha, error_offset))\n",
    "    \n",
    "  # 1. replay memory initial population\n",
    "  if 0 <= len(replay_memory) < replay_memory_init_size:\n",
    "    print('Replay memory initial population')\n",
    "    dqn_replay_memory_init(env, replay_memory, replay_memory_init_size, episode_max_steps)\n",
    "\n",
    "  # inizialize epsilon equal to max_epsilon\n",
    "  epsilon = data.get('epsilon',max_epsilon)\n",
    "  # initialize beta equal to min_beta\n",
    "  beta = data.get('beta', min_beta)\n",
    "  train_rewards = data.get('train_rewards',[])\n",
    "  train_step_count = len(train_rewards) #T\n",
    "\n",
    "  print('start training')\n",
    "  for n in range(train_step_count, episode_count): \n",
    "    # for visualization purposes\n",
    "    episode_start_time = time.time()    \n",
    "    episode_start_epsilon = epsilon\n",
    "    episode_start_beta = beta\n",
    "    # episode init\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    step_count = 0  #t\n",
    "    done = False\n",
    "\n",
    "    while step_count < episode_max_steps and not done: \n",
    "      # decide whether to pick a random action or to exploit the already computed Q-values\n",
    "      if random.uniform(0, 1) <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "      else:\n",
    "        # 4. stacked frame normalization\n",
    "        norm_state = state/255.0\n",
    "        q_values = dqn_action_model.predict(norm_state[np.newaxis], verbose=False)\n",
    "        action = np.argmax(q_values)\n",
    "            \n",
    "      # take the action and observe the outcome state and reward\n",
    "      new_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "      # store transition in the replay memory\n",
    "      replay_memory.append([state, action, reward, new_state, done])\n",
    "            \n",
    "      # update the current state\n",
    "      state = new_state\n",
    "                        \n",
    "      if train_step_count % step_per_update == 0 and len(replay_memory) >= batch_size:\n",
    "        # get a random mini-batch from the replay memory\n",
    "        *mini_batch, weights, indexes = replay_memory.sample_minibatch(batch_size, beta)\n",
    "        # 7. update the action model weights\n",
    "        losses = double_dqn_update(dqn_action_model, dqn_target_model, mini_batch, gamma, weights)\n",
    "        replay_memory.update_priorities(indexes, losses)\n",
    "                \n",
    "      if train_step_count % step_per_update_target_model ==0:\n",
    "          # copy weights from action to target model\n",
    "          dqn_target_model.set_weights(dqn_action_model.get_weights())\n",
    "\n",
    "      # reduce epsilon\n",
    "      epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
    "      # increase beta\n",
    "      beta = min(max_beta, beta + beta_increase)\n",
    "            \n",
    "      # increase episode step count and total step count\n",
    "      step_count += 1\n",
    "      train_step_count += 1\n",
    "\n",
    "      # add the current reward to the episode total reward\n",
    "      episode_reward += reward \n",
    "        \n",
    "    # put the episode total reward into a list for visualization purposes\n",
    "    train_rewards.append(episode_reward)\n",
    "\n",
    "    if n > 0 and n % save_freq == 0 and save_folder is not None:\n",
    "      save_model(save_folder, dqn_action_model, dqn_target_model, train_rewards, epsilon, beta, replay_memory)\n",
    "\n",
    "    # for visualization purposes\n",
    "    episode_finish_time=time.time()\n",
    "    episode_elapsed_time=episode_finish_time-episode_start_time\n",
    "    episode_avg_step_time=episode_elapsed_time/step_count\n",
    "    moving_avg_reward=statistics.mean(train_rewards[-moving_avg_window_size:])\n",
    "\n",
    "    print_episode_info(n,\n",
    "                       step_count,\n",
    "                       train_step_count,\n",
    "                       episode_start_epsilon,\n",
    "                       epsilon,\n",
    "                       episode_start_beta,\n",
    "                       beta,\n",
    "                       episode_elapsed_time,\n",
    "                       episode_avg_step_time,\n",
    "                       episode_reward,\n",
    "                       moving_avg_reward)        \n",
    "    # condition to consider the task solved\n",
    "    if moving_avg_stop_thr is not None and moving_avg_reward > moving_avg_stop_thr:\n",
    "      break\n",
    "\n",
    "  # return a list containing the total rewards of all training episodes\n",
    "  return train_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_TOo7ZB5Aks"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnv1rssCy3xM"
   },
   "source": [
    "### Iperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_Mk2Cb-BCjNf"
   },
   "outputs": [],
   "source": [
    "episode_count = 10              # Total number of training episodes\n",
    "episode_max_steps= 10_000            # Maximum number of steps per episode\n",
    "save_freq = 50                      # model save frequency  \n",
    "load_folder = save_folder = 'DQN_pong' # folder to save data\n",
    "top_crop = 30                       # y coordinate of the top of the region of interest\n",
    "bottom_crop = 195                   # y coordinate of the bottom of the region of interest\n",
    "left_crop = 5                       # x coordinate of the left of the region of interest\n",
    "right_crop = 155                    # x coordinate of the right of the region of interest\n",
    "\n",
    "crops = (top_crop, right_crop, bottom_crop, left_crop)\n",
    "shape = (84, 84)\n",
    "\n",
    "frame_skip = 4  # how many frames skip\n",
    "stacked_frames = 4 # how many frames stack together \n",
    "\n",
    "replay_memory_max_size = 100_000    # The maximum number of transitions stored into the replay memory. The Deepmind paper suggests 1M however this may cause memory issues.\n",
    "replay_memory_init_size = 20_000    # The maximum number of transitions stored into the replay memory. The Deepmind paper suggests 50K.\n",
    "batch_size = 32                     # The mini-batch size\n",
    "\n",
    "step_per_update = 4                 # The number of total steps executed between successive updates of the action model weights\n",
    "step_per_update_target_model = 10_000  # The number of total steps executed between successive replaces of the target model weights\n",
    "\n",
    "max_epsilon = 1.0                     # Exploration probability at start\n",
    "min_epsilon = 0.1                     # Minimum exploration probability\n",
    "epsilon_decay = (max_epsilon-min_epsilon) / 1_000_000.0  # Decay for exploration probability\n",
    "\n",
    "max_beta = 1  # how much compensate the bias at the end\n",
    "min_beta = 0.4 # how much compensate the bias at the start\n",
    "beta_increase = (max_epsilon-min_epsilon) / 1_000_000.0 # increase for beta\n",
    "\n",
    "alpha = 0.6 # how much weight \n",
    "error_offset = 0.1\n",
    "\n",
    "gamma = 0.99                        # Discount factor\n",
    "moving_avg_window_size = 100          # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
    "moving_avg_stop_thr = 21              # Minimum value of the total reward moving average to consider the task solved\n",
    "action_repeat_probability = 0.25    # probability to repeat last action instead the action\n",
    "learning_rate = 2.5e-4              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk786yLQzO17"
   },
   "source": [
    "Make the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1O1kWvg0zQib"
   },
   "outputs": [],
   "source": [
    "env = make_env(env_name, stacked_frames, frame_skip, action_repeat_probability, shape, crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujUznSEwzXox"
   },
   "source": [
    "create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "q0jf31OizRPV"
   },
   "outputs": [],
   "source": [
    "# To train dueling use build_dueling_dqn\n",
    "dqn_action_model=build_dueling_dqn(action_count=env.action_space.n)\n",
    "dqn_target_model=build_dueling_dqn(action_count=env.action_space.n)\n",
    "dqn_target_model.set_weights(dqn_action_model.get_weights())\n",
    "\n",
    "loss = keras.losses.Huber(reduction='none')\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate = learning_rate, clipnorm = 1.0)\n",
    "dqn_action_model.compile(optimizer = optimizer, loss = loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FOosbvl33zmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(dqn_action_model, to_file='model_strucure.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95A3EK71zzCb"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "p54QP3ADz0DE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: No file or directory found at DQN_pong/dqn_action_model\n",
      "Data not loaded\n",
      "Replay memory initial population\n",
      "19999/20000\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 14:15:19.217751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2023-04-03 14:15:19.538841: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Steps: 842[842] Epsilon: [1.000->0.999] Beta: [0.400->0.401] Time: 45.7s[0.05s] Total reward: -20.0[-20.0]\n",
      "Episode: 1 Steps: 825[1667] Epsilon: [0.999->0.998] Beta: [0.401->0.402] Time: 41.9s[0.05s] Total reward: -21.0[-20.5]\n",
      "Episode: 2 Steps: 1066[2733] Epsilon: [0.998->0.998] Beta: [0.402->0.402] Time: 54.9s[0.05s] Total reward: -18.0[-19.7]\n",
      "Episode: 3 Steps: 825[3558] Epsilon: [0.998->0.997] Beta: [0.402->0.403] Time: 42.6s[0.05s] Total reward: -21.0[-20.0]\n",
      "Episode: 4 Steps: 793[4351] Epsilon: [0.997->0.996] Beta: [0.403->0.404] Time: 40.5s[0.05s] Total reward: -21.0[-20.2]\n",
      "Episode: 5 Steps: 872[5223] Epsilon: [0.996->0.995] Beta: [0.404->0.405] Time: 44.9s[0.05s] Total reward: -21.0[-20.3]\n",
      "Episode: 6 Steps: 765[5988] Epsilon: [0.995->0.995] Beta: [0.405->0.405] Time: 39.4s[0.05s] Total reward: -21.0[-20.4]\n",
      "Episode: 7 Steps: 871[6859] Epsilon: [0.995->0.994] Beta: [0.405->0.406] Time: 44.8s[0.05s] Total reward: -20.0[-20.4]\n",
      "Episode: 8 Steps: 978[7837] Epsilon: [0.994->0.993] Beta: [0.406->0.407] Time: 50.6s[0.05s] Total reward: -19.0[-20.2]\n",
      "Episode: 9 Steps: 843[8680] Epsilon: [0.993->0.992] Beta: [0.407->0.408] Time: 43.9s[0.05s] Total reward: -20.0[-20.2]\n"
     ]
    }
   ],
   "source": [
    "train_rewards = dqn_training(episode_count,\n",
    "                             env,\n",
    "                             dqn_action_model,\n",
    "                             dqn_target_model,\n",
    "                             episode_max_steps,\n",
    "                             replay_memory_max_size,\n",
    "                             replay_memory_init_size,\n",
    "                             alpha,\n",
    "                             error_offset,\n",
    "                             batch_size,\n",
    "                             step_per_update,\n",
    "                             step_per_update_target_model,\n",
    "                             max_epsilon,\n",
    "                             min_epsilon,\n",
    "                             epsilon_decay,\n",
    "                             gamma,\n",
    "                             max_beta,\n",
    "                             min_beta,\n",
    "                             beta_increase,\n",
    "                             moving_avg_window_size,\n",
    "                             moving_avg_stop_thr,\n",
    "                             load_folder,\n",
    "                             save_folder,\n",
    "                             save_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huXDrEQt3BhS"
   },
   "outputs": [],
   "source": [
    "#!zip -r dqn_train.zip DQN_breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkMUj4_-OgKm"
   },
   "source": [
    "# Stable Baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCT0LbS8yd0I"
   },
   "source": [
    "## Introduzione agli algoritmi **Policy-Based**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CvJ3mEDx5Ao"
   },
   "source": [
    "Nei metodi **policy-based** invece di apprendere una *value function* che calcola il _valore atteso_ della somma dei rewards dato uno stato ed un'azione si apprende direttamente la policy che mappa da stato ad azione. \n",
    "Questo significa che si ottimizza direttamente la policy $\\pi$.\n",
    "\n",
    "La policy potrebbe essere deterministica o stocastica.\n",
    "In questo progetto si tratterà solamente la versione stocastica.\n",
    "Una policy stocastica restituisce una distribuzione di probabilità sulle azioni:\n",
    "\n",
    "<p><img src=\"https://cdn-media-1.freecodecamp.org/images/1*YCABimP7x1wZZZKqz2CoyQ.png\" width=\"400\"/></p>\n",
    "\n",
    "Questo significa che ogni possibile azione ha una certa probabilità per essere effettuata. Questo viene definito **Partially Observable Markov Decision Process**.\n",
    "\n",
    "I modelli policy-based hanno diversi vantaggi:\n",
    "\n",
    "- hanno migliori proprietà di convergenza. Rispetto ai metodi _value-based_, in cui ci sono oscillazioni dovute alla stima degli _action values_, i policy-based ottimizzano i parametri utilizzando il gradiente per effettuare aggiornamenti piu \"delicati\" ad ogni step. Dato che si segue il gradiente si ha la certezza di convergere in un minimo (sia locale che globale). Inoltre è più facile da approssimare che una value function (e.g. in Pong è difficile assegnare uno score al movimento della racchetta)\n",
    "\n",
    "<p><img src=\"https://cdn-media-1.freecodecamp.org/images/1*0lYcY5TBSqfNwdu8TduB6g.png\" width=\"400\"/></p>\n",
    "\n",
    "- sono più efficaci in spazi dimensionali molto grandi o quando si usano azioni continue. Con DQN si calcola un valore per ogni possibile azione, ma se le azioni fossero infinite o molte non sarebbe possibile. I metodi policy-based trovano i valori ottimi in modo iterativo ottimizzando i parametri.\n",
    "- stocastic policy, mentre _value-based_ non possono. Questo comporta che\n",
    "  -  non è necessario implementare un sistema di _exploration/exploitation_.\n",
    "  -  Elimina il _perceptual aliasing_, un fenomeno per il quale due stati sembrano gli stessi ma necessitano di azioni differenti. In questi casi un metodo _value-based_ pensando che i due stati siano uguali sceglierebbe, in uno dei due casi, sempre l'azione sbagliata in modo deterministico (questo effetto si riduce con la _epsilon greedy strategy_, però nelle fasi avanzate dell'addestramento perde la sua efficacia). La conseguenza è che l'agente potrebbe spendere un sacco di tempo per continuare correttamente. Una policy stocastica ottimale effettuerebbe un'azione in modo random cosi che l'agente raggiunga l'obbiettivo con una probabilità più alta.\n",
    "  - efficace nei casi in cui un approccio deterministico non sarebbe ottimo (e.g. sasso-carta-forbici)\n",
    "I metodi policy-based hanno un grosso svantaggio: spesso convergono in un minimo locale invece del massimo globale. Diversamente da DQN, che cerca sempre il massimo, questi metodi convergono step by step lentamente, il che potrebbe portare a tempi di addestramenti più lunghi. \n",
    "\n",
    "## Policy Gradient Theorem\n",
    "Sia $x$ l'azione che si andrà ad intraprendere, $s$ lo stato attuale e $\\theta$ i pesi della rete. La rete darà in output delle probabilità $p(x \\mid s; \\theta)$ che dipendono dallo stato $s$ e dai pesi $\\theta$. Si vuole quindi massimizzare la _reward attesa_ $r(x)$. $r(x)$ dipende dall'azione $x$ che si sceglie.\n",
    "\n",
    "$$ J(\\theta) = E_{x \\sim p(x \\mid s; \\theta)}[r(x)]$$\n",
    "\n",
    "Per massimizzare la funzione si effettua il _gradient ascent_. Per farlo occorre $\\nabla_\\theta J(\\theta)$.\n",
    "\n",
    "<p><img src=\"https://www.sitolorenzogardini.altervista.org/images/expectation_gradient.PNG\" width=\"600\"/></p>\n",
    "\n",
    "**Da questo risulta che è il valore atteso della reward moltiplicato per il logaritmo delle probabilità delle azioni**\n",
    "\n",
    "Questo vale però per una singola azione. Ora si considerano delle traiettorie, una sequenza di azioni.\n",
    "\n",
    "### Traiettorie\n",
    "\n",
    "Si assume il principio della _Markov Property_: lo stato successivo $s'$ dipende dallo stato corrente $s$ e dall'azione $a$ ma dati $(s,a)$ è indipendente da tutti gli stati e le azioni precedenti.\n",
    "La probabilità di una traiettoria è definita come:\n",
    "\n",
    "<p><img src=\"https://www.sitolorenzogardini.altervista.org/images/transition_prob.PNG\" width=\"500\"/></p>\n",
    "\n",
    "Considerando un processo di generazione stocastica che genera delle traiettorie $(s, a, r)$:\n",
    "\n",
    "$$\\tau = (s_0, a_0, r_0, s_1, a_1, r_1,...,s_{T-1}, a_{T-1}, r_{T-1}, s_T)$$\n",
    "\n",
    "Si vuole calcolare il valore atteso delle rewards su una traiettoria:\n",
    "\n",
    "<p><img src=\"https://www.sitolorenzogardini.altervista.org/images/expected_reward_trajectory_unfolded.PNG\" width=\"400\"/></p>\n",
    "\n",
    "dove $R(\\tau)$ è semplicemente la somma delle rewards della traiettoria.\n",
    "\n",
    "$$R(\\tau) = \\sum\\limits_{t=0}^{T-1}r_t$$\n",
    "\n",
    "### Probabilità di una traiettoria data una policy\n",
    "\n",
    "<p><img src=\"https://www.sitolorenzogardini.altervista.org/images/probability_of_trajectory_given_policy.PNG\n",
    "\" width=\"600\"/></p>\n",
    "\n",
    "### Massimizzare il valore eatteso del ritorno di una traiettoria\n",
    "\n",
    "Dato che \n",
    "\n",
    "$$\\log p(\\tau \\mid \\theta) = log \\;\\mu(s_0) +  \\sum\\limits_{t=0}^{T-1} \\Big[ log \\; \\pi (a_t \\mid s_t, \\theta) + log \\; p(s_{t+1},r_t \\mid s_t, a_t) \\Big]$$\n",
    "\n",
    "il $\\nabla_\\theta \\log p(\\tau \\mid \\theta)$, diventa :\n",
    "\n",
    "$$\\nabla_{\\theta} \\sum\\limits_{t=0}^{T-1} log \\; \\pi (a_t \\mid s_t, \\theta)$$\n",
    "\n",
    "dato che è l'unico termine che dipende da $\\theta$. Quindi \n",
    "\n",
    "$$\\nabla_{\\theta} \\mathbb{E}_{\\tau} [R(\\tau)] = \\mathbb{E}_{\\tau} \\Big[ R(\\tau) \\nabla_{\\theta} \\sum\\limits_{t=0}^{T-1} log \\; \\pi (a_t \\mid s_t, \\theta) \\Big]$$\n",
    "\n",
    "\n",
    "Da notare che non dipende dalla distribuzione di probabilità dell'environment (quello che sopra era riportato come transition function, $p(s_{t+1},r_t \\mid s_t, a_t)$). Per questo i modelli Gradient policy vengono chiamati **Model Free Models**.\n",
    "\n",
    "### Policy gradient ascent (esempio con REINFORCE)\n",
    "\n",
    "L'aggiornamento dei pesi della rete viene effettuato in modo iterativo in questo modo:\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta + \\alpha \\gamma^t G \\nabla_\\theta \\; ln \\; \\pi (a_t \\mid s_t, \\theta)$$\n",
    "\n",
    "dove $G$ è il return dello step $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaBZMgG7yawu"
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmVskxfex5Qy"
   },
   "source": [
    "Formula del gradiente della policy:\n",
    "\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*YQqZyAJ1QehPXFW36TKwmw.webp\" width='300'/></p>\n",
    "\n",
    "Questo approccio ha diversi problemi: _noisy gradient_ e una variabilità molto alta. Questo è dato dal fatto che i _samples_ possono avere molta variabilità tra loro e quindi anche tra le rewards. Questo causa _noisy gradient_ ed un conseguente apprendimento instabile. Inoltre le rewards possono essere **sparse**, la maggior parte delle azioni ha come reward 0. Questo causa un rallentamento nell'addestramento.\n",
    "\n",
    "## Baseline\n",
    "Per introdurre stabilità si utilizza una baseline:\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*45Wfsmp3KnzbJPKWt7GXpg.webp\" width='400'/></p>\n",
    "\n",
    "Questo produce gradienti più piccoli e quindi aggiornamenti più piccoli e più stabili.\n",
    "\n",
    "Questo verrà combinato successivamente.\n",
    "\n",
    "## Actor-Critic Method\n",
    "\n",
    "Si può decomporre $G_t$ come:\n",
    "\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*p0R0jWEaUAk2CEo-rk7MrQ.webp\" width='500'/></p>\n",
    "\n",
    "Il secondo valore atteso combacia con il _Q value_:\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*1CcRr_bsaebzGXyi6gC6yA.webp\" width='300'/></p>\n",
    "\n",
    "I grandiente diventa:\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*JYp-uQrMJKEHadx4RBrR1A.webp\" width='500'/></p>\n",
    "\n",
    "dove $Q_w$ è una rete con pesi $w$.\n",
    "\n",
    "Questo porta al metodo _**Actor-Critic**_:\n",
    "\n",
    "- Il _Critic_ stima la _value function_. Questo potrebbe essere il valore dell'azione (il valore _Q_ ) o il valore dello stato (il valore _V_ ).\n",
    "- L' _Actor_ aggiorna la distribuzione della policy nella direzione suggerita dal _Critic_.\n",
    "\n",
    "Sia _Critic_ che _Actor_ sono delle reti neruali, nella formula di prima il _Critic_ è dato dalla funzione _Q_, quindi viene chiamato _**Q Actor-Critic**_.\n",
    "\n",
    "Algoritmo:\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*BVh9xq3VYEsgz6eNB3F6cA.webp\" width='500'/></p>\n",
    "\n",
    "## Advantage Actor Critic\n",
    "\n",
    "Come baselines $b(s_t)$ si utilizza la value function $V(s_t)$\n",
    "\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*GjirmHTNdxHgo1Z8iQjDbg.webp\" width='300'/></p>\n",
    "\n",
    "Questo si può interpretare come quanto sia migliore intraprendere l'azione $a$ rispetto a quella scelta dalla policy $\\pi$.\n",
    "\n",
    "_Q_ si può scrivere rispetto a _V_.\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*ffw4Ri8eyB5FUArQq9wZeg.webp\" width='300'/></p>\n",
    "\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*pgGvGTyJ7gtlCICD5M-Z-A.webp\" width='400'/></p>\n",
    "\n",
    "Il gradiente diventa:\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*pjE0o_wWTdcjprdDQFnwog.webp\" width='400'/></p>\n",
    "\n",
    "## Implementazione\n",
    "\n",
    "Introdotta la versione A3C da Google nel 2016.\n",
    "https://arxiv.org/pdf/1602.01783.pdf\n",
    "\n",
    "In sostanza, A3C implementa la formazione parallela in cui più lavoratori in ambienti paralleli aggiornano in modo indipendente una funzione di valore globale , quindi \"asincrona\". Uno dei principali vantaggi di avere attori asincroni è l'esplorazione efficace ed efficiente dello spazio degli stati.\n",
    "\n",
    "<p><img src=\"https://miro.medium.com/max/1100/1*EiFOXH3MZ0r_a_aKLd9r8A.webp\" width='400'/></p>\n",
    "\n",
    "## A2C vs A3C\n",
    "A2C funziona come A3C ma senza la parte asincrona. Questo significa che è presente un coordinatore per i vari agenti. È stato dimostrato empiricamente che A2C ha performance comparabili, se non migliori, della controparte asincrona. \n",
    "https://openai.com/blog/baselines-acktr-a2c/.\n",
    "\n",
    "Da qui si evidenzia il fatto che non ci siano miglioramenti nella versione asyncrona. _\"This A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies\"_.\n",
    "\n",
    "<p><img src=\"https://www.researchgate.net/publication/344581679/figure/fig1/AS:945026666336258@1602323323378/Asynchronous-learning-with-A2C-and-A3C-algorithms.ppm\" width='400'/></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Be5fYbzy5JW"
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOVAupv5y6g-"
   },
   "source": [
    "https://arxiv.org/pdf/1707.06347v2.pdf\n",
    "\n",
    "L'idea con Proximal Policy Optimization (PPO) è che si vuole migliorare la stabilità dell'addestramento della policy limitando le modifiche apportate alla policy in ogni epoca di training: si vuole evitare di effettuare aggiornamenti troppo grandi. Questo per due motivi:\n",
    "\n",
    "- aggiornamenti più piccoli della policy hanno maggiori probabilità di convergere in una soluzione ottima\n",
    "\n",
    "- un aggiornamento troppo grande può comportare quello che viene chiamato 'falling off the cliff' (caduta dal precipizio) in cui la policy impiega molto tempo per recuperare oppure diverge completamente.\n",
    "\n",
    "<p><img src='https://huggingface.co/blog/assets/93_deep_rl_ppo/cliff.jpg' width='200'/></p>\n",
    "\n",
    "In PPO la policy viene aggiornata in modo conservativo. Per farlo occorre misurare quanto un aggiornamento si è discostato da quello precedente. Questo valore viene poi _clippato_ in un range $[1 - \\epsilon, 1 + \\epsilon]$ in modo che la policy attuale non si discosti troppo da quella vecchia (da qui il termine _proximal policy_).\n",
    "\n",
    "## Clipped Surrogate Objective\n",
    "\n",
    "La funzione obbiettivo della policy vanilla ha il problema di essere lenta a convergere ed è troppo variabile in fase di addestramento.\n",
    "\n",
    "Si introduce una nuova funzione che introduce il _clipping_ per evitare aggiornamenti troppo repentini della policy.\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/clipped_surrogate_objective.PNG' width='700'/></p>\n",
    "\n",
    "### Ratio $\\rho_t(\\theta)$\n",
    "Il _ratio_ corrisponde alla probabilità di scegliere l'azione $a_t$ nello stato $s_t$ nella policy attuale diviso da quella precedente:\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/ratio.PNG' width='300'/></p>\n",
    "\n",
    "- se $r_t(\\theta) > 1$ l'azione $a_t$ nello stato $s_t$ è più probabile nella policy corrente che in quella vecchia\n",
    "- se $r_t(\\theta) \\in [0, 1]$ l'azione è meno probabile nella policy corrente che in quella vecchia.\n",
    "\n",
    "Questo permette di stimare la divergenza tra la policy vecchia e quella nuova.\n",
    "\n",
    "Questo rapporto sistituisce il $\\log \\pi_\\theta(a_t | s_t)$ della funzione obbiettivo base. \n",
    "\n",
    "<p><img src='https://miro.medium.com/max/1476/0*S949lemw0fEDVPJE' width='400'/></p>\n",
    "\n",
    "Questo però non basta, in quanto se un'azione presa è molto più probabile nella policy corrente causerebbe un aggiornamento eccessivo della policy. \n",
    "\n",
    "## Clipped Surrogate Objective function\n",
    "\n",
    "Si vincola quindi la funzione obbiettivo in modo che il _ratio_ non si scosti di molto da 1 (nel paper $r_t (\\theta) \\in [0.8, 1.2]$). In questo modo ci si assicura che non ci siano aggiornamenti della policy troppo grandi perchè la nuova policy non può essere troppo diversa da quella vecchia.\n",
    "\n",
    "<p><img src='https://miro.medium.com/max/695/1*rSpNhA8WkB0QqefM3vBYtA.png' width='400'/></p>\n",
    "\n",
    "Si configurano questi casi:\n",
    "\n",
    "<p><img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*8kQ1WK6jgJcZw8HdPwzKDg.png' width='500'/></p>\n",
    "\n",
    "Quando la funzione obiettivo viene clippata il gradiente diventa 0 (dato che $1 - \\epsilon$ non dipende da $\\theta$). Cosi facendo vengono scoraggiate tutti gli aggiornamenti o troppo grandi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ygk7hb94t5TJ"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "#import imageio\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "# models\n",
    "from stable_baselines3.a2c import A2C\n",
    "from stable_baselines3.ppo import PPO\n",
    "from stable_baselines3 import DQN\n",
    "# stable wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.sb2_compat.rmsprop_tf_like import RMSpropTFLike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbqS_u5koaud"
   },
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "J11XbaBShRr4"
   },
   "outputs": [],
   "source": [
    "class StickyActionEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, action_repeat_probability: float) -> None:\n",
    "    super().__init__(env)\n",
    "    self.action_repeat_probability = action_repeat_probability\n",
    "\n",
    "  def reset(self, **kwargs):\n",
    "    self._sticky_action = 0  # NOOP\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "  def step(self, action: int):\n",
    "    if self.np_random.random() >= self.action_repeat_probability:\n",
    "      self._sticky_action = action\n",
    "    return self.env.step(self._sticky_action)\n",
    "\n",
    "class SkipEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, skip: int = 4) -> None:\n",
    "    super().__init__(env)\n",
    "    self._skip = skip\n",
    "\n",
    "  def step(self, action: int):\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    for _ in range(self._skip):\n",
    "      obs, reward, done, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "          break\n",
    "    return obs, total_reward, done, info\n",
    "  \n",
    "class PreprocessImageEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, screen_size=84, crops=(None, None, None, None)) -> None:\n",
    "    super().__init__(env)\n",
    "    self.screen_size = screen_size\n",
    "    self.crops = crops\n",
    "    self.observation_space = spaces.Box(\n",
    "        low=0, \n",
    "        high=255, \n",
    "        shape=(screen_size, screen_size, 1), \n",
    "        dtype=env.observation_space.dtype)\n",
    "\n",
    "  def step(self, action):\n",
    "    obs, reward, game_over, info = self.env.step(action)\n",
    "    return self._process(obs), reward, game_over, info\n",
    "\n",
    "  def reset(self):\n",
    "    return self._process(self.env.reset())\n",
    "\n",
    "  def _process(self, obs):\n",
    "    gray = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "    height, width = gray.shape\n",
    "\n",
    "    top_crop, right_crop, bottom_crop, left_crop = self.crops\n",
    "    top_crop = 0 if top_crop is None else top_crop\n",
    "    right_crop = width if right_crop is None else right_crop\n",
    "    bottom_crop = height if bottom_crop is None else bottom_crop\n",
    "    left_crop = 0 if left_crop is None else left_crop\n",
    "\n",
    "    cut_obs = gray[top_crop:bottom_crop, left_crop:right_crop]\n",
    "    resized = cv2.resize(cut_obs, (self.screen_size, self.screen_size), interpolation=cv2.INTER_NEAREST)\n",
    "    int_image = np.asarray(resized, dtype=np.uint8)\n",
    "    return np.expand_dims(int_image, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "58nh5qx4ocG7"
   },
   "outputs": [],
   "source": [
    "def create_training_env(env_name, \n",
    "                 n_envs=1,\n",
    "                 action_repeat_probability=0.25, \n",
    "                 stack_frames=4, \n",
    "                 frame_skip=4,\n",
    "                 crops = [None, None, None, None],\n",
    "                 monitor_path=None, \n",
    "                 difficulty=1):\n",
    "  \n",
    "  def make_env(env_name, index=0):\n",
    "    env = gym.make(env_name, difficulty=difficulty)\n",
    "    env = StickyActionEnv(env, action_repeat_probability)\n",
    "    env = SkipEnv(env, frame_skip)\n",
    "    env = PreprocessImageEnv(env, crops=crops)\n",
    "    return env\n",
    "\n",
    "  vec_env = DummyVecEnv([lambda: make_env(env_name, i) for i in range(n_envs)])\n",
    "  stacked_vec_env = VecFrameStack(vec_env,  n_stack=stack_frames)\n",
    "  return VecMonitor(stacked_vec_env, monitor_path)\n",
    "\n",
    "\n",
    "def make_gif(model, gif_name):\n",
    "  images = []\n",
    "  obs = model.env.reset()\n",
    "  img = model.env.render(mode=\"rgb_array\")\n",
    "  while(True):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, done, _ = model.env.step(action)\n",
    "    img = model.env.render(mode=\"rgb_array\")\n",
    "    if done:\n",
    "      break\n",
    "  imageio.mimsave(f\"{gif_name}.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s35FKBwDVh_Y"
   },
   "source": [
    "Iperparametri ottimizzati per ciascun algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "atRffRg7-9Xf"
   },
   "outputs": [],
   "source": [
    "time_stamps = 100_000\n",
    "env_name = 'PongNoFrameskip-v4'\n",
    "stack_frames = 4\n",
    "difficulty = 3\n",
    "action_repeat_probability = -1\n",
    "crops = [34, None, 194, None]\n",
    "frame_skip = 4\n",
    "\n",
    "# ppo_max_env = 8\n",
    "# a2c_max_env = 16\n",
    "# dqn_max_env = 16\n",
    "\n",
    "dqn_hyperparams = {\n",
    "  'buffer_size': 100_000,\n",
    "  'learning_rate': 1e-4,\n",
    "  'batch_size': 32,\n",
    "  'learning_starts': 100_000,\n",
    "  'target_update_interval': 1000,\n",
    "  'train_freq': 4,\n",
    "  'gradient_steps': 1,\n",
    "  'exploration_fraction': 0.1,\n",
    "  'exploration_final_eps': 0.01,\n",
    "}\n",
    "\n",
    "ppo_hyperparams = {\n",
    "  'n_steps': 128,\n",
    "  'n_epochs': 4,\n",
    "  'batch_size': 256,\n",
    "  'learning_rate': 2.5e-4,\n",
    "  'clip_range': 0.1,\n",
    "  'vf_coef': 0.5,\n",
    "  'ent_coef': 0.01,\n",
    "}\n",
    "\n",
    "a2c_hyperparams = {\n",
    "  'ent_coef': 0.01,\n",
    "  'vf_coef': 0.25,\n",
    "  'policy_kwargs': dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))\n",
    "}\n",
    "\n",
    "algo = DQN\n",
    "n_envs = 16\n",
    "hyper_params = dqn_hyperparams\n",
    "save_path = f'{algo.__name__}/{algo.__name__}_{n_envs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNLYxdTmVmCD"
   },
   "source": [
    "creazione dell'env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "GnGtg4VvS7-4"
   },
   "outputs": [],
   "source": [
    "training_env = create_training_env(env_name, \n",
    "                   n_envs, \n",
    "                   action_repeat_probability, \n",
    "                   stack_frames, \n",
    "                   frame_skip, \n",
    "                   crops,\n",
    "                   monitor_path=save_path, \n",
    "                   difficulty=difficulty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd02RJZDVtNM"
   },
   "source": [
    "Addestramento dell'algoritmo selezionato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ZY-Tj0tDTLfT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 765      |\n",
      "|    ep_rew_mean      | -21.0    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1        |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 12240    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | -21.0    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2        |\n",
      "|    fps             | 1199     |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 12240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | -21.0    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3        |\n",
      "|    fps             | 1199     |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 12240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | -21.0    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 1199     |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 12240    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 771      |\n",
      "|    ep_rew_mean      | -21.0    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5        |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 12688    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 783      |\n",
      "|    ep_rew_mean      | -21.0    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6        |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 13504    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 793      |\n",
      "|    ep_rew_mean      | -21.0    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 7        |\n",
      "|    fps              | 1195     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 13648    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 808        |\n",
      "|    ep_rew_mean      | -20.777779 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 8          |\n",
      "|    fps              | 1193       |\n",
      "|    time_elapsed     | 11         |\n",
      "|    total_timesteps  | 13776      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 808        |\n",
      "|    ep_rew_mean     | -20.777779 |\n",
      "| time/              |            |\n",
      "|    episodes        | 9          |\n",
      "|    fps             | 1193       |\n",
      "|    time_elapsed    | 11         |\n",
      "|    total_timesteps | 13776      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 816      |\n",
      "|    ep_rew_mean      | -20.8    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 10       |\n",
      "|    fps              | 1193     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 14160    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 822        |\n",
      "|    ep_rew_mean      | -20.727272 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 11         |\n",
      "|    fps              | 1192       |\n",
      "|    time_elapsed     | 11         |\n",
      "|    total_timesteps  | 14224      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 832        |\n",
      "|    ep_rew_mean      | -20.666666 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 12         |\n",
      "|    fps              | 1192       |\n",
      "|    time_elapsed     | 12         |\n",
      "|    total_timesteps  | 14912      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 841        |\n",
      "|    ep_rew_mean      | -20.615385 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 13         |\n",
      "|    fps              | 1192       |\n",
      "|    time_elapsed     | 12         |\n",
      "|    total_timesteps  | 15344      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 851        |\n",
      "|    ep_rew_mean      | -20.571428 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 14         |\n",
      "|    fps              | 1192       |\n",
      "|    time_elapsed     | 13         |\n",
      "|    total_timesteps  | 15584      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 862        |\n",
      "|    ep_rew_mean      | -20.533333 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 15         |\n",
      "|    fps              | 1192       |\n",
      "|    time_elapsed     | 13         |\n",
      "|    total_timesteps  | 16320      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 875      |\n",
      "|    ep_rew_mean      | -20.4375 |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1192     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 17136    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 870        |\n",
      "|    ep_rew_mean      | -20.470589 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 17         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 20         |\n",
      "|    total_timesteps  | 24768      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 865      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 18       |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 25376    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.473684 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 19         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 21         |\n",
      "|    total_timesteps  | 26016      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 863      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 26848    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 866       |\n",
      "|    ep_rew_mean      | -20.47619 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 21        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 22        |\n",
      "|    total_timesteps  | 26960     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 864      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 22       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 27152    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | -20.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 23       |\n",
      "|    fps             | 1197     |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 27152    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | -20.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 1197     |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 27152    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 863      |\n",
      "|    ep_rew_mean      | -20.52   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 25       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 27360    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 862      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 26       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 27696    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 859        |\n",
      "|    ep_rew_mean      | -20.518518 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 27         |\n",
      "|    fps              | 1196       |\n",
      "|    time_elapsed     | 23         |\n",
      "|    total_timesteps  | 27824      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 862      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 28960    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 864       |\n",
      "|    ep_rew_mean      | -20.48276 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 29        |\n",
      "|    fps              | 1197      |\n",
      "|    time_elapsed     | 24        |\n",
      "|    total_timesteps  | 29792     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.466667 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 30         |\n",
      "|    fps              | 1196       |\n",
      "|    time_elapsed     | 25         |\n",
      "|    total_timesteps  | 30096      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 862        |\n",
      "|    ep_rew_mean      | -20.483871 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 31         |\n",
      "|    fps              | 1196       |\n",
      "|    time_elapsed     | 25         |\n",
      "|    total_timesteps  | 30208      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 867       |\n",
      "|    ep_rew_mean      | -20.46875 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 1196      |\n",
      "|    time_elapsed     | 25        |\n",
      "|    total_timesteps  | 30464     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 864        |\n",
      "|    ep_rew_mean      | -20.484848 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 33         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 30         |\n",
      "|    total_timesteps  | 37008      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 864        |\n",
      "|    ep_rew_mean      | -20.470589 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 34         |\n",
      "|    fps              | 1200       |\n",
      "|    time_elapsed     | 32         |\n",
      "|    total_timesteps  | 39120      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 861        |\n",
      "|    ep_rew_mean      | -20.485714 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 35         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 32         |\n",
      "|    total_timesteps  | 39200      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 860      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 39520    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 859        |\n",
      "|    ep_rew_mean      | -20.513514 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 37         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 33         |\n",
      "|    total_timesteps  | 39840      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 857        |\n",
      "|    ep_rew_mean      | -20.526316 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 38         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 33         |\n",
      "|    total_timesteps  | 40512      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 857        |\n",
      "|    ep_rew_mean      | -20.538462 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 39         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 34         |\n",
      "|    total_timesteps  | 40896      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 857      |\n",
      "|    ep_rew_mean      | -20.525  |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 41072    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 857        |\n",
      "|    ep_rew_mean      | -20.536585 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 41         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 34         |\n",
      "|    total_timesteps  | 41280      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 859       |\n",
      "|    ep_rew_mean      | -20.52381 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 42        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 35        |\n",
      "|    total_timesteps  | 42064     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 857        |\n",
      "|    ep_rew_mean      | -20.534883 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 43         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 35         |\n",
      "|    total_timesteps  | 42336      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 856        |\n",
      "|    ep_rew_mean      | -20.545454 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 44         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 35         |\n",
      "|    total_timesteps  | 42480      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 858        |\n",
      "|    ep_rew_mean      | -20.533333 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 45         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 35         |\n",
      "|    total_timesteps  | 42544      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 858        |\n",
      "|    ep_rew_mean      | -20.543478 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 46         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 36         |\n",
      "|    total_timesteps  | 44112      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 859        |\n",
      "|    ep_rew_mean      | -20.531916 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 47         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 37         |\n",
      "|    total_timesteps  | 44960      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 863      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 45872    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.489796 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 49         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 42         |\n",
      "|    total_timesteps  | 50480      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 862      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 50       |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 53056    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 862        |\n",
      "|    ep_rew_mean      | -20.509804 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 51         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 44         |\n",
      "|    total_timesteps  | 53152      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 861       |\n",
      "|    ep_rew_mean      | -20.51923 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 44        |\n",
      "|    total_timesteps  | 53200     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 862        |\n",
      "|    ep_rew_mean      | -20.509434 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 53         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 45         |\n",
      "|    total_timesteps  | 54032      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 860        |\n",
      "|    ep_rew_mean      | -20.518518 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 54         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 45         |\n",
      "|    total_timesteps  | 54304      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 859        |\n",
      "|    ep_rew_mean      | -20.527273 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 55         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 46         |\n",
      "|    total_timesteps  | 55200      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 857        |\n",
      "|    ep_rew_mean      | -20.535715 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 56         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 46         |\n",
      "|    total_timesteps  | 55232      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 859        |\n",
      "|    ep_rew_mean      | -20.526316 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 57         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 46         |\n",
      "|    total_timesteps  | 55744      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 860      |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 58       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 55840    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 860        |\n",
      "|    ep_rew_mean      | -20.491526 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 59         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 47         |\n",
      "|    total_timesteps  | 56560      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.483334 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 60         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 47         |\n",
      "|    total_timesteps  | 57376      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 868        |\n",
      "|    ep_rew_mean      | -20.442623 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 61         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 48         |\n",
      "|    total_timesteps  | 58352      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 868        |\n",
      "|    ep_rew_mean      | -20.451612 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 62         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 48         |\n",
      "|    total_timesteps  | 58448      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 868        |\n",
      "|    ep_rew_mean      | -20.444445 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 63         |\n",
      "|    fps              | 1197       |\n",
      "|    time_elapsed     | 49         |\n",
      "|    total_timesteps  | 59824      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 870      |\n",
      "|    ep_rew_mean      | -20.4375 |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1197     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 60016    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 868        |\n",
      "|    ep_rew_mean      | -20.446154 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 65         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 52         |\n",
      "|    total_timesteps  | 62720      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 868        |\n",
      "|    ep_rew_mean      | -20.454546 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 66         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 55         |\n",
      "|    total_timesteps  | 66400      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 867        |\n",
      "|    ep_rew_mean      | -20.447762 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 67         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 56         |\n",
      "|    total_timesteps  | 67504      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 866        |\n",
      "|    ep_rew_mean      | -20.455883 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 68         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 56         |\n",
      "|    total_timesteps  | 68080      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 866        |\n",
      "|    ep_rew_mean      | -20.463768 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 69         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 56         |\n",
      "|    total_timesteps  | 68256      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.471428 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 70         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 57         |\n",
      "|    total_timesteps  | 68368      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.464788 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 71         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 57         |\n",
      "|    total_timesteps  | 68976      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 867        |\n",
      "|    ep_rew_mean      | -20.444445 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 72         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 57         |\n",
      "|    total_timesteps  | 69040      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 866        |\n",
      "|    ep_rew_mean      | -20.452055 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 73         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 57         |\n",
      "|    total_timesteps  | 69248      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 868        |\n",
      "|    ep_rew_mean      | -20.445946 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 74         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 57         |\n",
      "|    total_timesteps  | 69264      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 868      |\n",
      "|    ep_rew_mean      | -20.44   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 75       |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 69520    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 867        |\n",
      "|    ep_rew_mean      | -20.447369 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 76         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 58         |\n",
      "|    total_timesteps  | 70064      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 866        |\n",
      "|    ep_rew_mean      | -20.454546 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 77         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 59         |\n",
      "|    total_timesteps  | 71136      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.461538 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 78         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 59         |\n",
      "|    total_timesteps  | 71552      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.468355 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 79         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 61         |\n",
      "|    total_timesteps  | 73216      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 864      |\n",
      "|    ep_rew_mean      | -20.475  |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 74960    |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.469135 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 81         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 62         |\n",
      "|    total_timesteps  | 75488      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 864        |\n",
      "|    ep_rew_mean      | -20.475609 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 82         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 66         |\n",
      "|    total_timesteps  | 79408      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 864        |\n",
      "|    ep_rew_mean      | -20.481928 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 83         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 67         |\n",
      "|    total_timesteps  | 81056      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.488094 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 84         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 68         |\n",
      "|    total_timesteps  | 81936      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.482353 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 85         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 68         |\n",
      "|    total_timesteps  | 82144      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 864        |\n",
      "|    ep_rew_mean      | -20.476744 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 86         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 68         |\n",
      "|    total_timesteps  | 82272      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 862       |\n",
      "|    ep_rew_mean      | -20.48276 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 87        |\n",
      "|    fps              | 1199      |\n",
      "|    time_elapsed     | 68        |\n",
      "|    total_timesteps  | 82304     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 862        |\n",
      "|    ep_rew_mean      | -20.477272 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 88         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 68         |\n",
      "|    total_timesteps  | 82480      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 862       |\n",
      "|    ep_rew_mean      | -20.47191 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 89        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 69        |\n",
      "|    total_timesteps  | 82736     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.455555 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 90         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 69         |\n",
      "|    total_timesteps  | 83808      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 864       |\n",
      "|    ep_rew_mean      | -20.43956 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 91        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 70        |\n",
      "|    total_timesteps  | 84240     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.434782 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 92         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 70         |\n",
      "|    total_timesteps  | 84656      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 865        |\n",
      "|    ep_rew_mean      | -20.430107 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 93         |\n",
      "|    fps              | 1198       |\n",
      "|    time_elapsed     | 70         |\n",
      "|    total_timesteps  | 84912      |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 864       |\n",
      "|    ep_rew_mean      | -20.43617 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 94        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 71        |\n",
      "|    total_timesteps  | 85456     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 864       |\n",
      "|    ep_rew_mean      | -20.43158 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 95        |\n",
      "|    fps              | 1198      |\n",
      "|    time_elapsed     | 71        |\n",
      "|    total_timesteps  | 85920     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 863      |\n",
      "|    ep_rew_mean      | -20.4375 |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 87200    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 864       |\n",
      "|    ep_rew_mean      | -20.42268 |\n",
      "|    exploration_rate | 0.01      |\n",
      "| time/               |           |\n",
      "|    episodes         | 97        |\n",
      "|    fps              | 1199      |\n",
      "|    time_elapsed     | 75        |\n",
      "|    total_timesteps  | 90528     |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 864        |\n",
      "|    ep_rew_mean      | -20.418367 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 98         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 77         |\n",
      "|    total_timesteps  | 92880      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/            |            |\n",
      "|    ep_len_mean      | 863        |\n",
      "|    ep_rew_mean      | -20.424242 |\n",
      "|    exploration_rate | 0.01       |\n",
      "| time/               |            |\n",
      "|    episodes         | 99         |\n",
      "|    fps              | 1199       |\n",
      "|    time_elapsed     | 79         |\n",
      "|    total_timesteps  | 94800      |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 863      |\n",
      "|    ep_rew_mean      | -20.43   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 95280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 864      |\n",
      "|    ep_rew_mean      | -20.42   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 101      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 95808    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 864      |\n",
      "|    ep_rew_mean      | -20.42   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 102      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 96048    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 865      |\n",
      "|    ep_rew_mean      | -20.41   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 103      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 96656    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 867      |\n",
      "|    ep_rew_mean      | -20.4    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 97600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 869      |\n",
      "|    ep_rew_mean      | -20.39   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 105      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 97632    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 870      |\n",
      "|    ep_rew_mean      | -20.38   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 106      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 97776    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 870      |\n",
      "|    ep_rew_mean      | -20.38   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 107      |\n",
      "|    fps              | 1199     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 97856    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 870      |\n",
      "|    ep_rew_mean      | -20.38   |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 1198     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 98432    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f8e29f4da00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = algo('CnnPolicy', training_env, **hyper_params, verbose=True)\n",
    "model.learn(time_stamps, log_interval=1)\n",
    "#model.save(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a gif with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_gif(model, f'{algo.__name__}_{n_envs}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gif con il risultato dell'addestramento con DQN e 1 environment\n",
    "\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/DQN_gif.gif' width='300'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szN5UYgkt1Bm"
   },
   "source": [
    "# Tensorflow Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8ARE9oyt4Og"
   },
   "source": [
    "Tensorflow Agents permette di creare un ambiente completo di reinforcement learning in modo modulare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm1wNB9l041j"
   },
   "source": [
    "<p><img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pJ5utPNBZFcJRhdxX-Fsfw.png' width='500'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUiv1B2xuXkv"
   },
   "source": [
    "Installazione e import di librerie base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "OlKaFGTLtDFU"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw9dtoRUwMCv"
   },
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "8sAxjwccvNOv"
   },
   "outputs": [],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zQTnGJ5hnwj"
   },
   "source": [
    "## Environment Wrappers and Atari Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go885qJfubhS"
   },
   "source": [
    "I wrappers utilizzati presentati nella parte manuale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "cF3RSG-pqteV"
   },
   "outputs": [],
   "source": [
    "class SkipEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, skip: int = 4) -> None:\n",
    "    super().__init__(env)\n",
    "    self._skip = skip\n",
    "\n",
    "  def step(self, action: int):\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    for _ in range(self._skip):\n",
    "      obs, reward, done, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "          break\n",
    "    return obs, total_reward, done, info\n",
    "\n",
    "class TFWrapper(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env):\n",
    "    super().__init__(env)\n",
    "    self.game_over = False\n",
    "\n",
    "  def reset(self):\n",
    "    self.game_over = False\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    if done:\n",
    "      self.game_over = True\n",
    "    return obs, reward, done, info\n",
    "  \n",
    "class PreprocessImageEnv(gym.Wrapper):\n",
    "  def __init__(self, env: gym.Env, screen_size=84, crops=(None, None, None, None)) -> None:\n",
    "    super().__init__(env)\n",
    "    self.screen_size = screen_size\n",
    "    self.crops = crops\n",
    "    self.observation_space = spaces.Box(\n",
    "        low=0, \n",
    "        high=255, \n",
    "        shape=(screen_size, screen_size, 1), \n",
    "        dtype=env.observation_space.dtype)\n",
    "\n",
    "  def step(self, action):\n",
    "    obs, reward, game_over, info = self.env.step(action)\n",
    "    return self._process(obs), reward, game_over, info\n",
    "\n",
    "  def reset(self):\n",
    "    return self._process(self.env.reset())\n",
    "\n",
    "  def _process(self, obs):\n",
    "    gray = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "    height, width = gray.shape\n",
    "\n",
    "    top_crop, right_crop, bottom_crop, left_crop = self.crops\n",
    "    top_crop = 0 if top_crop is None else top_crop\n",
    "    right_crop = width if right_crop is None else right_crop\n",
    "    bottom_crop = height if bottom_crop is None else bottom_crop\n",
    "    left_crop = 0 if left_crop is None else left_crop\n",
    "\n",
    "    cut_obs = gray[top_crop:bottom_crop, left_crop:right_crop]\n",
    "    resized = cv2.resize(cut_obs, (self.screen_size, self.screen_size), interpolation=cv2.INTER_NEAREST)\n",
    "    int_image = np.asarray(resized, dtype=np.uint8)\n",
    "    return np.expand_dims(int_image, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLro9QSqukND"
   },
   "source": [
    "Creazione dell'env. Quest'ultimo deve essere wrappato con `TFPyEnvironment` in modo che possa essere ottimizzato con TF.\n",
    "\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/env_creation.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "50DlRAi25Nkj"
   },
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from functools import partial\n",
    "from tf_agents.environments import atari_wrappers\n",
    "from tf_agents.environments import suite_gym \n",
    "from tf_agents.environments.suite_gym import wrap_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "stack_frames = 4\n",
    "difficulty = 3\n",
    "action_repeat_probability = 0.25\n",
    "crops = [34, None, 194, None]\n",
    "frame_skip = 4\n",
    "seed = 1234\n",
    "screen_size = 84\n",
    "max_episode_steps = 25_000 # <=> 100k ALE frames since 1 step = 4 frames\n",
    "env_name = 'PongNoFrameskip-v4'\n",
    "dueling = True\n",
    "double = True\n",
    "save_path = 'tf_' + ('DDQN' if double else 'DQN') + '/' + ('dueling_' if dueling else '') + 'rewards.csv'\n",
    "\n",
    "\n",
    "def wrap(gym_env, wrappers, max_episode_steps):\n",
    "  wrapped = suite_gym.wrap_env(\n",
    "      gym_env,\n",
    "      max_episode_steps=max_episode_steps,\n",
    "      gym_env_wrappers=wrappers,\n",
    "      time_limit_wrapper=atari_wrappers.AtariTimeLimit,\n",
    "      spec_dtype_map = {gym.spaces.Box: np.uint8},\n",
    "      auto_reset=False)\n",
    "  return TFPyEnvironment(wrapped)\n",
    "\n",
    "env = gym.make(env_name, difficulty=difficulty)\n",
    "env.seed(seed)\n",
    "\n",
    "tf_training_env = wrap(env, \n",
    "                    [TFWrapper,\n",
    "                     partial(SkipEnv, skip=frame_skip),\n",
    "                     partial(PreprocessImageEnv, crops=crops),\n",
    "                     FrameStack4,\n",
    "                     partial(Monitor, filename=save_path)],\n",
    "                    max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiqiUIJMhImD"
   },
   "source": [
    "## Q-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C_Z6kmxuw5X"
   },
   "source": [
    "Qui vengono definite le due versioni della rete, una classica DQN convoluzionale (creata facilmente utilizzando la classe `QNetwork`) mentre la seconda Dueling (questa viene wrappata da un layer `Sequential` per renderla compatibile con TF agents).\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/network_creation.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "oGtsVvb05apx"
   },
   "outputs": [],
   "source": [
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tf_agents.networks import Network\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "def normal_dqn(env):\n",
    "  preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "  conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "  fc_layer_params=[512]\n",
    "  return QNetwork(\n",
    "      env.observation_spec(),\n",
    "      env.action_spec(),\n",
    "      preprocessing_layers=preprocessing_layer,\n",
    "      conv_layer_params=conv_layer_params,\n",
    "      fc_layer_params=fc_layer_params\n",
    "  )\n",
    "\n",
    "def dueling_dqn(input_shape=(84, 84, 4), action_count=9):\n",
    "  input = Input(shape=input_shape, name='input')\n",
    "  preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)(input)\n",
    "  conv_1 = Conv2D(filters=32, kernel_size=8, strides=4, padding='valid', activation='relu')(preprocessing_layer)\n",
    "  conv_2 = Conv2D(filters=64, kernel_size=4, strides=2, padding='valid', activation='relu')(conv_1)\n",
    "  conv_3 = Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu')(conv_2)\n",
    "  flatten = Flatten()(conv_3)\n",
    "  # branch state\n",
    "  flatten_state = Dense(512, activation='relu')(flatten)\n",
    "  state = Dense(1, activation='linear')(flatten_state)\n",
    "  # branch advantages\n",
    "  flatten_advantages = Dense(512, activation='relu')(flatten)\n",
    "  raw_advantages = Dense(action_count,activation='linear')(flatten_advantages) \n",
    "\n",
    "  # action score - mean action score = centered action score\n",
    "  advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1, keepdims=True) \n",
    "  Q_values = state + advantages\n",
    "  model = keras.Model(inputs=[input], outputs=[Q_values])\n",
    "  q_net = sequential.Sequential([model])\n",
    "  return q_net\n",
    "\n",
    "q_net = dueling_dqn(action_count=env.action_space.n) if dueling else normal_dqn(tf_training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBqgnn74lSnZ"
   },
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vNHgjOOvKDt"
   },
   "source": [
    "Qui viene inizializzato l'agente tramite le classi `DqnAgent` e `DdqnAgent`.\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/agent_creation.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "MynZZQagDqf-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......lambda\n",
      ".........vars\n",
      "......tf_op_lambda\n",
      ".........vars\n",
      "......tf_op_lambda_1\n",
      ".........vars\n",
      "......tf_op_lambda_2\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-04-03 14:27:21     13209200\n",
      "metadata.json                                  2023-04-03 14:27:21           64\n",
      "config.json                                    2023-04-03 14:27:21         5829\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-04-03 14:27:20     13209200\n",
      "metadata.json                                  2023-04-03 14:27:20           64\n",
      "config.json                                    2023-04-03 14:27:20         5829\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......lambda\n",
      ".........vars\n",
      "......tf_op_lambda\n",
      ".........vars\n",
      "......tf_op_lambda_1\n",
      ".........vars\n",
      "......tf_op_lambda_2\n",
      ".........vars\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent, DdqnAgent\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # train the model every 4 steps\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate = 2.5e-4, clipnorm = 1.0)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=1_000_000,\n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "klass = DdqnAgent if double else DqnAgent\n",
    "agent = klass(tf_training_env.time_step_spec(),\n",
    "                 tf_training_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=1_000,\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDzYHkDZlY7y"
   },
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUVP43shvUsI"
   },
   "source": [
    "Qui viene creato il buffer. La libreria non permette di usare Prioritized Experience Replay.\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/replay_\n",
    "buffer_creation.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "wrQDlkGOk0L5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 14:27:21.511515: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2822400000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "# https://gist.github.com/Kenneth-Schroeder/6dbdd4e165331164e0d9dcc2355698e2\n",
    "# from tf_prioritized_replay_buffer import TFPrioritizedReplayBuffer\n",
    "\n",
    "'''\n",
    "replay_buffer = TFPrioritizedReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "'''\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_training_env.batch_size,\n",
    "    max_length=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVQn_nW4ntNW"
   },
   "source": [
    "## Observers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqY8te9BxBQU"
   },
   "source": [
    "Qui vengono creati gli observers. Questi vengono notificati dal Driver con le _trajectories_.\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/observers.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "yPDYY1dIlqjW"
   },
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch\n",
    "\n",
    "training_metrics = [\n",
    "  tf_metrics.NumberOfEpisodes(),\n",
    "  tf_metrics.EnvironmentSteps(),\n",
    "  tf_metrics.AverageReturnMetric(),\n",
    "  tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWzGsGJMoxrV"
   },
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84qKLN6exZUs"
   },
   "source": [
    "I _Drivers_ campionano, in base alla policy specificata, le _trajectories_ dagli _environments_. Vengono creati due `DynamicStepDriver`:\n",
    "- `init_driver`: utilizza una `RandomTFPolicy` per inizializzare la reply memory\n",
    "- `collect_driver`: utilizza la policy del modello per eseguire le azioni nell'env e collezionare *trajectories*\n",
    "\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/driver_creation.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "xxR4fZSloq9F"
   },
   "outputs": [],
   "source": [
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "#initialization\n",
    "initial_collect_policy = RandomTFPolicy(tf_training_env.time_step_spec(),\n",
    "                                        tf_training_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_training_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer_observer],\n",
    "    num_steps=20_000)\n",
    "\n",
    "# policy\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_training_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + training_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srHksL-9sYQM"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV3s1uKwx4ds"
   },
   "source": [
    "Genera i dati attingendo dal _replay buffer_, li prefetcha per velocizzare l'addestramento. Qui `num_steps` viene messo a due in modo da fare il sampling di due _trajectories_ consegutive. Questo perchè una trajectory possiede solamente lo stato attuale ma non quello successivo, che invece è richiesto da DQN per addestrarsi.\n",
    "<p><img src='https://www.sitolorenzogardini.altervista.org/images/dataset_creation.png' width='400'/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "bCAjW7zKpWeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/studentiDL/lorenzo/dl/lib/python3.8/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From /home/studentiDL/lorenzo/dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "WARNING:tensorflow:From /home/studentiDL/lorenzo/dl/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CexrxvvwYoc"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnvva1ZbyLvf"
   },
   "source": [
    "Le funzioni `run` dei driver e `train` dell'agente vengono vrappate con `function` di tensorflow in modo che possano essere ottimizzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "09Pgqf47ufPE"
   },
   "outputs": [],
   "source": [
    "from tf_agents.utils.common import function\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "init_driver.run = function(init_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGLvrWaHyfhj"
   },
   "source": [
    "Qui la reply memory viene inizializzata campionando casualmente l'environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ksFXf7i2x7ve"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replay memory init\n"
     ]
    }
   ],
   "source": [
    "# init memory\n",
    "print('replay memory init')\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjeTvPo1zvKv"
   },
   "source": [
    "Funzione di addestramento della rete. Il corpo corrisponde al classico algoritmo di apprendimento di DQN:\n",
    "1) si collezionano steps fino a che non bisogna aggiornare la rete\n",
    "2) si aggiorna la rete\n",
    "3) si ripetono i passi 1 e 2 fino a che non vengono effettuate le interazioni specificate da `n_interations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "FdbfecrMufmA"
   },
   "outputs": [],
   "source": [
    "def train_agent(n_iterations):\n",
    "  time_step = None\n",
    "  policy_state = agent.collect_policy.get_initial_state(tf_training_env.batch_size)\n",
    "  iterator = iter(dataset)\n",
    "  for iteration in range(n_iterations):\n",
    "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "    trajectories, _ = next(iterator)\n",
    "    agent.train(trajectories)\n",
    "    if iteration % 100 == 0:\n",
    "      log_metrics(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "s4Uxw_qlwb45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/studentiDL/lorenzo/dl/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 14:28:37.193757: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 4\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 404\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 804\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 1\n",
      "\t\t EnvironmentSteps = 1204\n",
      "\t\t AverageReturn = -21.0\n",
      "\t\t AverageEpisodeLength = 807.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 1\n",
      "\t\t EnvironmentSteps = 1604\n",
      "\t\t AverageReturn = -21.0\n",
      "\t\t AverageEpisodeLength = 807.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 2\n",
      "\t\t EnvironmentSteps = 2004\n",
      "\t\t AverageReturn = -20.5\n",
      "\t\t AverageEpisodeLength = 863.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 2\n",
      "\t\t EnvironmentSteps = 2404\n",
      "\t\t AverageReturn = -20.5\n",
      "\t\t AverageEpisodeLength = 863.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 3\n",
      "\t\t EnvironmentSteps = 2804\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 893.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 3\n",
      "\t\t EnvironmentSteps = 3204\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 893.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 3\n",
      "\t\t EnvironmentSteps = 3604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 893.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 4\n",
      "\t\t EnvironmentSteps = 4004\n",
      "\t\t AverageReturn = -19.75\n",
      "\t\t AverageEpisodeLength = 927.25\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 4\n",
      "\t\t EnvironmentSteps = 4404\n",
      "\t\t AverageReturn = -19.75\n",
      "\t\t AverageEpisodeLength = 927.25\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 5\n",
      "\t\t EnvironmentSteps = 4804\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 922.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 5\n",
      "\t\t EnvironmentSteps = 5204\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 922.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 6\n",
      "\t\t EnvironmentSteps = 5604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 900.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 6\n",
      "\t\t EnvironmentSteps = 6004\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 900.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 6\n",
      "\t\t EnvironmentSteps = 6404\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 900.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 7\n",
      "\t\t EnvironmentSteps = 6804\n",
      "\t\t AverageReturn = -19.85714340209961\n",
      "\t\t AverageEpisodeLength = 921.4285888671875\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 7\n",
      "\t\t EnvironmentSteps = 7204\n",
      "\t\t AverageReturn = -19.85714340209961\n",
      "\t\t AverageEpisodeLength = 921.4285888671875\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 8\n",
      "\t\t EnvironmentSteps = 7604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 909.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 8\n",
      "\t\t EnvironmentSteps = 8004\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 909.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 9\n",
      "\t\t EnvironmentSteps = 8404\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 913.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 9\n",
      "\t\t EnvironmentSteps = 8804\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 913.6666870117188\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 10\n",
      "\t\t EnvironmentSteps = 9204\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 919.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 10\n",
      "\t\t EnvironmentSteps = 9604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 919.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 10\n",
      "\t\t EnvironmentSteps = 10004\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 919.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 11\n",
      "\t\t EnvironmentSteps = 10404\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 923.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 11\n",
      "\t\t EnvironmentSteps = 10804\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 923.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 12\n",
      "\t\t EnvironmentSteps = 11204\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 929.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 12\n",
      "\t\t EnvironmentSteps = 11604\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 929.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 13\n",
      "\t\t EnvironmentSteps = 12004\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 932.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 13\n",
      "\t\t EnvironmentSteps = 12404\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 932.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 13\n",
      "\t\t EnvironmentSteps = 12804\n",
      "\t\t AverageReturn = -19.799999237060547\n",
      "\t\t AverageEpisodeLength = 932.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 14\n",
      "\t\t EnvironmentSteps = 13204\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 919.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 14\n",
      "\t\t EnvironmentSteps = 13604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 919.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 15\n",
      "\t\t EnvironmentSteps = 14004\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 920.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 15\n",
      "\t\t EnvironmentSteps = 14404\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 920.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 16\n",
      "\t\t EnvironmentSteps = 14804\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 918.0999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 16\n",
      "\t\t EnvironmentSteps = 15204\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 918.0999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 17\n",
      "\t\t EnvironmentSteps = 15604\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 907.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 17\n",
      "\t\t EnvironmentSteps = 16004\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 907.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 18\n",
      "\t\t EnvironmentSteps = 16404\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 910.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 18\n",
      "\t\t EnvironmentSteps = 16804\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 910.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 18\n",
      "\t\t EnvironmentSteps = 17204\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 910.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 19\n",
      "\t\t EnvironmentSteps = 17604\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 900.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 19\n",
      "\t\t EnvironmentSteps = 18004\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 900.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 20\n",
      "\t\t EnvironmentSteps = 18404\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 888.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 20\n",
      "\t\t EnvironmentSteps = 18804\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 888.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 21\n",
      "\t\t EnvironmentSteps = 19204\n",
      "\t\t AverageReturn = -20.299999237060547\n",
      "\t\t AverageEpisodeLength = 881.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 21\n",
      "\t\t EnvironmentSteps = 19604\n",
      "\t\t AverageReturn = -20.299999237060547\n",
      "\t\t AverageEpisodeLength = 881.2000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 22\n",
      "\t\t EnvironmentSteps = 20004\n",
      "\t\t AverageReturn = -20.399999618530273\n",
      "\t\t AverageEpisodeLength = 876.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 22\n",
      "\t\t EnvironmentSteps = 20404\n",
      "\t\t AverageReturn = -20.399999618530273\n",
      "\t\t AverageEpisodeLength = 876.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 23\n",
      "\t\t EnvironmentSteps = 20804\n",
      "\t\t AverageReturn = -20.600000381469727\n",
      "\t\t AverageEpisodeLength = 857.0999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 23\n",
      "\t\t EnvironmentSteps = 21204\n",
      "\t\t AverageReturn = -20.600000381469727\n",
      "\t\t AverageEpisodeLength = 857.0999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 24\n",
      "\t\t EnvironmentSteps = 21604\n",
      "\t\t AverageReturn = -20.5\n",
      "\t\t AverageEpisodeLength = 857.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 24\n",
      "\t\t EnvironmentSteps = 22004\n",
      "\t\t AverageReturn = -20.5\n",
      "\t\t AverageEpisodeLength = 857.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 24\n",
      "\t\t EnvironmentSteps = 22404\n",
      "\t\t AverageReturn = -20.5\n",
      "\t\t AverageEpisodeLength = 857.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 25\n",
      "\t\t EnvironmentSteps = 22804\n",
      "\t\t AverageReturn = -20.299999237060547\n",
      "\t\t AverageEpisodeLength = 863.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 25\n",
      "\t\t EnvironmentSteps = 23204\n",
      "\t\t AverageReturn = -20.299999237060547\n",
      "\t\t AverageEpisodeLength = 863.9000244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 26\n",
      "\t\t EnvironmentSteps = 23604\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 871.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 26\n",
      "\t\t EnvironmentSteps = 24004\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 871.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 27\n",
      "\t\t EnvironmentSteps = 24404\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 872.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 27\n",
      "\t\t EnvironmentSteps = 24804\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 872.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 27\n",
      "\t\t EnvironmentSteps = 25204\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 872.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 28\n",
      "\t\t EnvironmentSteps = 25604\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 889.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 28\n",
      "\t\t EnvironmentSteps = 26004\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 889.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 29\n",
      "\t\t EnvironmentSteps = 26404\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 893.7999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 29\n",
      "\t\t EnvironmentSteps = 26804\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 893.7999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 30\n",
      "\t\t EnvironmentSteps = 27204\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 897.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 30\n",
      "\t\t EnvironmentSteps = 27604\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 897.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 31\n",
      "\t\t EnvironmentSteps = 28004\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 912.7999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 31\n",
      "\t\t EnvironmentSteps = 28404\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 912.7999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 31\n",
      "\t\t EnvironmentSteps = 28804\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 912.7999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 32\n",
      "\t\t EnvironmentSteps = 29204\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 904.5999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 32\n",
      "\t\t EnvironmentSteps = 29604\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 904.5999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 33\n",
      "\t\t EnvironmentSteps = 30004\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 912.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 33\n",
      "\t\t EnvironmentSteps = 30404\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 912.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 34\n",
      "\t\t EnvironmentSteps = 30804\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 907.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 34\n",
      "\t\t EnvironmentSteps = 31204\n",
      "\t\t AverageReturn = -19.899999618530273\n",
      "\t\t AverageEpisodeLength = 907.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 35\n",
      "\t\t EnvironmentSteps = 31604\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 886.0999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 35\n",
      "\t\t EnvironmentSteps = 32004\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 886.0999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 36\n",
      "\t\t EnvironmentSteps = 32404\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 901.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 36\n",
      "\t\t EnvironmentSteps = 32804\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 901.5\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 37\n",
      "\t\t EnvironmentSteps = 33204\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 895.4000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 37\n",
      "\t\t EnvironmentSteps = 33604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 895.4000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 37\n",
      "\t\t EnvironmentSteps = 34004\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 895.4000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 38\n",
      "\t\t EnvironmentSteps = 34404\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 894.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 38\n",
      "\t\t EnvironmentSteps = 34804\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 894.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 39\n",
      "\t\t EnvironmentSteps = 35204\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 899.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 39\n",
      "\t\t EnvironmentSteps = 35604\n",
      "\t\t AverageReturn = -20.0\n",
      "\t\t AverageEpisodeLength = 899.7000122070312\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 40\n",
      "\t\t EnvironmentSteps = 36004\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 889.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 40\n",
      "\t\t EnvironmentSteps = 36404\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 889.0\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 41\n",
      "\t\t EnvironmentSteps = 36804\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 881.5999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 41\n",
      "\t\t EnvironmentSteps = 37204\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 881.5999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 41\n",
      "\t\t EnvironmentSteps = 37604\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 881.5999755859375\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 42\n",
      "\t\t EnvironmentSteps = 38004\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 896.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 42\n",
      "\t\t EnvironmentSteps = 38404\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 896.2999877929688\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 43\n",
      "\t\t EnvironmentSteps = 38804\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 903.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 43\n",
      "\t\t EnvironmentSteps = 39204\n",
      "\t\t AverageReturn = -20.100000381469727\n",
      "\t\t AverageEpisodeLength = 903.9000244140625\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 44\n",
      "\t\t EnvironmentSteps = 39604\n",
      "\t\t AverageReturn = -20.200000762939453\n",
      "\t\t AverageEpisodeLength = 897.2000122070312\n"
     ]
    }
   ],
   "source": [
    "train_agent(10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JhWvAgdOLWl"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "RuosSv9R3kgF"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdsBpx6OI_BI"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9oxp0jpnBJP"
   },
   "source": [
    "`load_csv(path, skiprows)` carica from `path` il file _csv_ saltando le prime `skiprows` righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "o4TxR_3pmmsA"
   },
   "outputs": [],
   "source": [
    "def load_csv(path, skip_rows=1):\n",
    "  return pd.read_csv(path, skiprows=skip_rows, sep=',').r.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz42wrO2nrMB"
   },
   "source": [
    "`compute_moving_avg_rewards(rewards, moving_avg_window_size)` calcola la media mobile dell'array delle rewards `rewards` con una finestra grande `moving_avg_windows_size` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "xjBFX05lmnnW"
   },
   "outputs": [],
   "source": [
    "def compute_moving_avg_rewards(rewards, moving_avg_window_size=100):\n",
    "  return np.convolve(rewards, np.ones(moving_avg_window_size)/moving_avg_window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX9ExA1Fn6HY"
   },
   "source": [
    "`plot_rewards(data, label, limit_data=5000)` grafica le rewards specificate in `data` con la corrispondente `label`, limitando i dati con `limit_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "yMEK2WxbmohB"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(data, label):\n",
    "  plt.plot(data, label=[label])\n",
    "  plt.xlabel('Episodes')\n",
    "  plt.ylabel('Rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxowa6lXo7Hu"
   },
   "source": [
    "`first_time_value_reached(data, value)` restituisce l'indice della posizione in cui `value` è >= di `data[i]`, `min_threshold` serve per evitare che ci siano valori molto alti di media all'inizio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "U_-k-Z1impb1"
   },
   "outputs": [],
   "source": [
    "def first_time_value_reached(data, value):\n",
    "  index = np.argmax(data > value)\n",
    "  return None if index == 0 else index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT_FMM5fpmtN"
   },
   "source": [
    "`compute_speedup(initial_value, final_value)` calcola in percentuale l'incremento di `initial_value` rispetto a `final_value` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "dkfrpP8xmqTZ"
   },
   "outputs": [],
   "source": [
    "def compute_speedup(value_1, value_2):\n",
    "  if value_1 is None or value_2 is None:\n",
    "    return np.inf\n",
    "  min_value = min(value_1, value_2)\n",
    "  if min_value == 0:\n",
    "    return np.inf\n",
    "  return round( abs(value_1 - value_2) / min_value * 100, ndigits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZwMhKb7p6DO"
   },
   "source": [
    "`plot_together(list_of_csv, moving_avg_windows_size=100, limit_data=5000, first_value_reached=20)` grafica l'elenco specificato di csv in `list_of_csv`, computa di ciascuno di essi la moving average utilizzando come finestra `moving_avg_windows_size` e limitando i dati a `limit_data` elementi. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "woB4AYiqmsQm"
   },
   "outputs": [],
   "source": [
    "def plot_together(list_of_csv, limit_data=5000, moving_avg_windows_size=100, first_value_reached=20):\n",
    "  csvs = [load_csv(csv)[:limit_data] for csv in list_of_csv]\n",
    "  algos = [Path(csv).stem for csv in list_of_csv]\n",
    "  moving_avgs = np.array([compute_moving_avg_rewards(csv, moving_avg_windows_size) for csv in csvs])\n",
    "  data = {algo : averages for algo, averages in zip(algos, moving_avgs)}\n",
    "  indexes = {algo : first_time_value_reached(avg, first_value_reached) for algo, avg in zip(algos, moving_avgs)}\n",
    "  sorted_indexes = dict(sorted(indexes.items(), key=lambda x: (x[1] is None, x[1])))\n",
    "  color_seq = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "  print(f\"Moving average over {first_value_reached} for the first time:\")\n",
    "  for index, algo in sorted_indexes.items():\n",
    "    print(algo, index)\n",
    "  print()\n",
    "\n",
    "  print(\"Speed up:\")\n",
    "  for i, (algo, faster_avg) in enumerate(list(sorted_indexes.items())[:-1], start=1):\n",
    "    print(algo)\n",
    "    for algo, slower_avg in list(sorted_indexes.items())[i:]:\n",
    "      speed_up = compute_speedup(faster_avg, slower_avg)\n",
    "      print(f'\\t{algo}: {speed_up}%')\n",
    "\n",
    "  for algo, moving_avg in data.items():\n",
    "    plt.title('All models rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.plot(moving_avg)\n",
    "  \n",
    "  plt.legend(data.keys())\n",
    "\n",
    "  for color, index in zip(color_seq, indexes.values()):\n",
    "    if index is not None:\n",
    "      plt.axvline(x = index, color = color, linestyle = 'dashed')\n",
    "      plt.annotate(index, (index, first_value_reached))\n",
    "\n",
    "  plt.axhline(20, color = 'gray', linestyle = 'dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27k39QSxqR6R"
   },
   "source": [
    "`compare_last_n_games(list_of_csv, lasts_n_games=50)` mosta un confronto tra gli ultimi `lasts_n_games` caricando le rewards da `list_of_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "dP2NPJJwmtXu"
   },
   "outputs": [],
   "source": [
    "def compare_last_n_games(list_of_csv, lasts_n_games=50):\n",
    "  mean_last_n_games = [np.mean(load_csv(csv)[-lasts_n_games:]) for csv in list_of_csv]\n",
    "  dataframe = pd.DataFrame({\n",
    "      'average_score' : mean_last_n_games,\n",
    "      'algorithm' : [Path(csv).stem for csv in list_of_csv]\n",
    "  }).sort_values('average_score', ascending=False)\n",
    "  ax = sns.barplot(data=dataframe, x=f'average_score', y='algorithm')\n",
    "  ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V94f2f4rDaG"
   },
   "source": [
    "`filter_csv(regex='')` carica i _csv_ in base alla regex passata (N.B. quest'ultima non deve includere `.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "rWqVPQwJqzGI"
   },
   "outputs": [],
   "source": [
    "def filter_csv(regex=''):\n",
    "  return glob(regex + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IzsP7YPrSV2"
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTX6mh0FuIFa"
   },
   "source": [
    "Download delle rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A_EsYsFrXsQ"
   },
   "outputs": [],
   "source": [
    "!wget  https://sitolorenzogardini.altervista.org/csvs/rewards.zip\n",
    "!wget  https://sitolorenzogardini.altervista.org/csvs/fails.zip\n",
    "\n",
    "!unzip rewards.zip\n",
    "!unzip fails.zip\n",
    "\n",
    "!rm rewards.zip\n",
    "!rm fails.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-bRLX_euMlJ"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pe80ITt-l11"
   },
   "source": [
    "Caricamento per categoria dei file di configurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_do6FzWuDfy"
   },
   "outputs": [],
   "source": [
    "stable_baselines_files = filter_csv('sb3_*')\n",
    "stable_baselines_ok = [f'sb3_{algo}.csv' for algo in ['A2C_16', 'PPO_8', 'DQN_1']]\n",
    "stable_baselines_bad = [f'sb3_{algo}.csv' for algo in ['A2C_1', 'PPO_1', 'DQN_16']]\n",
    "fails = filter_csv('fail_*')\n",
    "tf_agents = filter_csv('tf_*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "480XzllA-pIc"
   },
   "source": [
    "Plot dei migliori risultati ottenuti utilizzando Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvNB8CR2sdeb"
   },
   "outputs": [],
   "source": [
    "plot_together(stable_baselines_ok, 8_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wqG0wg0_JtN"
   },
   "source": [
    "Plot degli algoritmi utilizzando gli algoritmi con un numero scorretto di evironments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Oys6N8gsjxZ"
   },
   "outputs": [],
   "source": [
    "plot_together(stable_baselines_bad, 8_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF8BjrEy_cL7"
   },
   "source": [
    "Confronto tra tutti gli algoritmi di Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pYMEmFixJ90"
   },
   "outputs": [],
   "source": [
    "plot_together(stable_baselines_files, 8_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqlquept_imi"
   },
   "source": [
    "Plot di tutti gli algoritmi che non hanno portato buoni risultati causati dallo `StickyActionEnv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhDgg17QtG4i"
   },
   "outputs": [],
   "source": [
    "plot_together(fails, 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNv65axsAYlr"
   },
   "source": [
    "Plot dei risultati ottenuti con Tensorflow Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCXWF8KFzIu3"
   },
   "outputs": [],
   "source": [
    "plot_together(tf_agents, 6_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1pWPWyHAeQq"
   },
   "source": [
    "Plot del confronto tra i migliori risultati tra Stable Baselines 3 e Tensorflow Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY27qiHjwD1o"
   },
   "outputs": [],
   "source": [
    "plot_together(stable_baselines_ok + ['tf_DDQN.csv', 'tf_DQN.csv'], limit_data=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48rwIc9ZAsj7"
   },
   "source": [
    "Media raggiunta nelle ultime N partite da ciascun algoritmo a confronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzAZHVUv1j0B"
   },
   "outputs": [],
   "source": [
    "compare_last_n_games(glob('*.csv'), lasts_n_games=50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-ASu-EG241qK",
    "tTiCG9w_pu1J",
    "2JhWvAgdOLWl"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8fbf9f74b8db9e2aa192069e803478403d7395270f7f8ae3b9a010c6b39919a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
